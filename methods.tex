\chapter{Methods}
    \section{Questionnaires}
	We have decided to complement our studies with a series of validated and widely used questionnaires.
	These tests could provide valuable insight into the interplay between psychotypes on one side and behavioural parameters or their neurophysiological underpinnings on the other.
	For test presentation we have decided on a web-based approach, for which we have used a web-enabled version of the free and open source (FOSS) software testMaker\cite{testmaker}.
	
	The choice of tests was based on both general relevance to the host group's research, and specific relevance to emotional perception.
	According to these criteria, we have compiled a selection of 8 questionnaires:
	\begin{itemize}
	    \item The Autism Spectrum Quotient questionnaire (\textbf{AQ}) \cite{Baron-Cohen2001}
	    \item The World Health Organization Adult ADHD Self-Report Scale (\textbf{ASRS}) \cite{Kessler2005}
	    \item The 1996 revised Beck Depression Inventory (\textbf{BDI2}) \cite{Beck1996}
	    \item The short version of the Borderline Symptoms List (\textbf{BSL23}) \cite{Bohus2009}
	    \item The Empathizing Quotient questionnaire (\textbf{EQ}) \cite{Baron-Cohen2004}
	    \item The Systemizing Quotient questionnaire (\textbf{SQ}) \cite{Baron-Cohen2003a}
	    \item The Emotion Regulation Questionnaire (\textbf{ERQ}) \cite{Gross2003}
	    \item The Schizotypal Personality Questionnaire (\textbf{SPQ}) \cite{Raine1991}
	\end{itemize}
	The questionnaires were administered in German language, and translated and validated versions were provided by the test databases of the Central Institute of Mental Health.
	Translation references are as follows:
	\begin{itemize}
	    \item \textbf{AQ} - Translated by C.M. Freitag, Homburg
	    \item \textbf{ASRS} - Translated by Dr. P. Kirsch, JLU Gießen 2005
	    \item \textbf{BDI2} - Translated by Pearson Assessment \& Information GmbH (Frankfurt/M. 2010)
	    \item \textbf{BSL23} - Translated at the Department of Psychosomatic Medicine and Psychotherapy, Central Institute of Mental Health.
	    \item \textbf{EQ} - Translated by Dipl.-Psych. Jörn de Haen
	    \item \textbf{SQ} - Translated by Dipl.-Psych. Jörn de Haen
	    \item \textbf{ERQ} - Translated by Brigit Abler and Henrik Kessler \cite{Abler2009}
	    \item \textbf{SPQ} - Tranlsated by Klein, Andersen, and Jahn \cite{Klein1997}
	\end{itemize}
	The questionnaires were evaluated with questioPy\cite{questiopy}, a Python script written for the purpose of this thesis and openly published on GitHub\cite{github}.
    \section{Behavioural Test}\label{sec:m_bt}
	In our current study we are exploring neuronal function ??ref in visual emotional perception.
	Our behavioural test of choice to tap into these systems relies on visual recognition and subsequent matching of emotion.
	We call our paradigm a Hariri-style behavioural test, in acknowledgement of the seminal implementations of a similar paradigm by Hariri and colleagues \cite{Hariri2000,Hariri2003}.
	    
	The paradigm consists of a triangular arrangement of three faces - with the bottom two aligned horizontally and the upper face displaced vertically and equidistant from both bottom faces.
	The behavioural test prompts the participant to observe a certain feature (in our case emotion, or - as described in section~\ref{sec:m_vs_si} - pixel arrangement) of the top face, and to select of the bottom faces the one repeating that feature.
	
	In our case bottom face selection was done via the arrow keys of a keyboard (in the preliminary experiments further described under section~\ref{sec:m_pe}) and via a remote control (in the fMRI trials further described under section~\ref{sec:m_fmri}).
	The participants received no sort of feedback after reacting to the individual items, and the images were continuously displayed independently of input for \SI{5}{\second} or \SI{4.5}{\second} (in the preliminary experiments), and for \SI{4}{\second} (in the fMRI trials); intertrial intervals showed a point or a cross and lasted for \SI{2}{\second} or approximately \SI{4}{\second} respectively.
	The higher inter-trial interval duration for the fMRI experiments was chosen to accomodate for two volume acquisitions, and we say “approximately” because a jitter was added (more on that in section~\ref{sec:m_fmri}). 
	
	During the above we take measures for valence (correct or incorrect) of responses, and for reaction times.
	
	\begin{wrapfigure}{r}{0.45\textwidth}
	  \centering
	    \scalebox{0.55}{\includegraphics{./img/vis_field.jpg}}
	    \caption{Perimetric map of the human field of view \cite{Ruch1960}.
	    For measurement the head and eyes were fixed, with the fovea pointing at \SI{0}{\degree} on the cross-hairs.
	    The white area affords binocular vision, the black area is completely outside the field of view.}
	    \label{fig:m_b_1}
	    \vspace{-1.0cm}
	\end{wrapfigure}
	
	In contrast to other formulations\cite{Hariri2000,Hariri2003} of the same paradigm (which keep all 3 stimuli equidistant),
	we have decided to adapt the positions of stimulus images to what we believe is a better fit for the human field of view.
	Simply aligning the midpoints of stimulus images to the vertices of an equilateral triangle presents some issues:
	\begin{itemize}
	    \item Not accounting for the aspect ratio of the actual stimulus images lessens control over the resulting populated visual window (pVW).
	    \item Horizontal and diagonal distances are not perceived identically by human vision 
	\end{itemize}
	
	We tackled the above by adapting the pVW to the normal human field of vision aspect ratio.
	To do so, we took the outermost non-neutral pixels (rather than the centers of the stimulus images) and used these as pVW delimiters in deciding where to position our images.
	
	For the following distance specifications we have defined the unit “u” as the height of our stimulus images.
	Seeing as our stimulus images have a fixed aspect ratio their width is thus consistently \SI[parse-numbers = false]{\frac{3}{4}}{u}.
	
	The static human field of view has a complex shape modulated amongst others by facial features (as seen in figure~\ref{fig:m_b_1}).
	If the outermost light paths fitting figure~\ref{fig:m_b_1} were to be circumscribed by a planar mask in the shape of the rectangle, its height would span approximately \SI{130}{\degree} and its width \SI{180}{\degree}.
	This gives the rectangular fit for the field of view a width-to-height ratio of 0.72(2).
	Given the data - and for numerical convenience - we have chosen to position the midpoints of our stimulus images as follows: 
	the distance between the two lower image midpoints is \SI{2}{u}; and the vertical offset of the top image midpoint from the horizontal line connecting the bottom image midpoints is \SI{1}{u}.
	This gives our pVW a total height of \SI{2}{u} and a total width of \SI[parse-numbers = false]{\frac{11}{4}}{u}.
	The resulting width-to-height ratio of 0.(72) was deemed a sufficiently close match for the afore mentioned human field of view (as approximated by a rectangle).
	We thus have reason to believe that our stimulus item distribution is superior to the one implemented by Hariri and colleagues, and we encourage further users of Hariri-style paradigms to note our observations.  
	
	We are aware that our simplified approximation still does not account for vertical saccades being slower\cite{TerryBahill1975} and less accurate\cite{Collewijn1988} than horizontal saccades, 
	nor for the fact that upward saccades tend to undershoot while downward saccades tend to overshoot \cite{Collewijn1988};
	however, integrating accurate metrics for these kinetics and determining their relevance for Hariri-style paradigms differs in focus from and far exceeds the scope of our current study.
	
	Our resulting pVW was scaled to fit the available display field of the monitor and tilted mirror set-up in our MRI scanner (more on this in section~\ref{sec:m_om_et}).
	The dimensions (specified in degrees of visual angle) are proportional to the ones mentioned above, scaled for u = \SI{4.24}{\degree}.
	The midpoint of the pVW (at \SI{0.5}{u} downward from the midpoint of the top image) was aligned to the midpoint of the monitor.
	A picture of this visual set-up can be seen in figure~\ref{fig:m_b_2}.
	
	\begin{figure}[!h]
	    \centering{\includegraphics[width=1\textwidth]{./img/01F_FE_C100_em100_composite.jpg}}
	    \caption{Sample image from the emotional face matching trials in our Hariri-style behavioural test.}
	    \label{fig:m_b_2}
	\end{figure}
		    
    \section{Visual Stimuli}
	\subsection{Emotional Faces}\label{sec:m_vs_ef}
	    The emotional face stimuli required by our behavioural test paradigm (see section~\ref{sec:m_bt}) have been sourced from the NimStim set of facial expressions \cite{Tottenham2009}.
	\subsection{Scrambled Images}\label{sec:m_vs_si}
	    In order to define a baseline of visual recognition we have decided to use trials in which visual input consists of the same pixels as in the emotional trials. 
	    To disentangle visual from semantic (emotion) identification we decided to use “scrambled” images - in which pixels from the emotional faces are permuted so as to obfuscate emotional expressions and facial features.
	    The recognition trials would thus entail matching identical permutations of the same picture.
	    For these trials the template and target will be the exact same image and the distractor will be a different permutation of the pixels which constitute the image the template and target were computed from. 
	    
	    There will be two sets of scrambled image trials: “easy” and “difficult” - to act as baselines for easy and difficult emotion recognition trials respectively.
	    Recognition difficulty of the scrambled image trials should scale proportionally with that of the easy and difficult emotion recognition trials (detailed under section~\ref{sec:m_vs_ef}).
	    To achieve this, scrambling has to be progressively complex.
	    Available data ??? suggests that scrambling of images via smaller sub-sections (further referred to as “clusters”) makes the images progressively difficult to identify and match to other copies.
	    This rationale determined the nature of our scrambling algorithms (detailed below) and was also experimentally validated in preliminary trials (as described in section~\ref{sec:r_pe}).
	    
	    Based on the preliminary results detailed in section~\ref{sec:r_pe} we have decided to use composite (kernel \textit{and} cluster based) scrambling for our experiment.
	    The scrambling kernel was constant at \SI{6}{\pixel} (as explained in section~\ref{sec:m_pe_cs}) 
	    and “hard” recognition trials had a scrambling cluster size of \SI{10}{\pixel} while “easy” trials had a scrambling cluster of \SI{22}{\pixel}.
	    
	    Scrambling was done via Pyxrand\cite{pyxrand}, home-brewed Python script written for the purpose of this thesis and openly published on GitHub.
	    The script provides both cluster-based and kernel-based scrambling:
	    \subsubsection{Cluster-Based Scrambling}
		This scrambling functionality recognises the face region of interest (ROI) by scanning for pixel lines with few unique values, and then divides the face ROI in square clusters of predefined sizes.
		The clusters then get permuted and rewritten in-place on the image - this is done via the \colorbox{vlg}{\texttt{montage2d}} function of the \colorbox{vlg}{\texttt{scikits\_image}} python package 
		(the function was written and contributed to the package for the benefit of the scientific community as part of this thesis).
		The image background is then filled with homogeneous values.
	    \subsubsection{Kernel-Based Scrambling}\label{sec:m_vs_si_kbs}
		This is done by remapping single pixels via the \colorbox{vlg}{\texttt{geometric\_transform}} function of the SciPy ecosystem \cite{scipy,Oliphant2007}.
		New positions are computed via a function which adds a random integer in the $[-K;K]$ ($K$ being the scrambling kernel integer) interval to both the X and Y coordinates of the said pixel.
		Effectively, this redistributes each pixel in an area of $[-K;K]$ around its original position with a standard deviation ($\sigma$) of $\approx 0.96K$ (as calculated in figure~\ref{eq:lrgn}).
    \section{Preliminary Experiments}\label{sec:m_pe}
	Preliminary experiments have been conducted in order to establish a proper paradigm for the main experiments of the project. 
	Their primary goal is determining which scrambling cluster sizes have reaction times comparable to the \SI{100}{\percent} and \SI{40}{\percent} emotional images (decided upon based on rationales delineated under section~\ref{sec:m_vs_ef}).
	
	For stimulus presentation and data acquisition in these experiments we used faceRT\cite{faceRT}, a home-brewed Python script written for the purpose of this thesis and openly published on GitHub.
	Our Python script uses the PsychoPy suite\cite{Peirce2008} for specialized, high-precision stimulus rendering and timing.
	To provide equal sample sizes (assumed by a number of statistical methods listed under section~\ref{sec:m_sa}) trials with no response present have not been excluded, but rather assigned a penalty reaction time of \SI{5}{\second}.
	Our data analysis script handles system flaws in reaction time recording (whereby negative reaction times are printed) by replacing the value with the median of the category of interest which the trial is part of (these categories are defined as detailed above by the nature of the presented stimulus).
	It should be noted that such errors appear only very seldom (at most twice in a measurement), so this slight manipulation via the most outlier-resistant predictor should impact the data in no other way than assuring its viability for statistical methods requiring full datasets.  
	
	Participants were recruited from staff at the Central Institute of Mental Health, and students of the Universities of Mannheim and Heidelberg.
	Since these are preparatory trials we opted for merely around 7 participants per run.
	Participants are identified in the study by a 2-letter code, and a subset of them have participated in multiple runs of our preliminary trials.
	It should be noted that runs are therefore not well suited for direct comparison or for binning; but they do offer a good predictor for expected variability in reaction times.
	
	For the following categories of experiments we have adhered to the behavioural test specifications under section~\ref{sec:m_bt}.  
	\subsection{Simple Cluster-Based Scrambling}\label{sec:m_pe_ss}
	    In a first set of trials we have scrambled images with cluster sizes starting at \SI{6}{\pixel} and progressing to \SI{26}{\pixel} in \SI{4}{\pixel} steps.
	    The stimulus presentation protocol used for these trials is revision \textcolor{lg}{ce2b3a8f30} of the faceRT\cite{faceRT} script suite.
	    Stimuli were presented fully randomized per-participant, and sequences were compiled manually, following the listed requirements:
	    \begin{itemize}
		\item Each condition should have 10 corresponding trials (conditions are: emotion \{happy; afraid\} $\times$ emotion intensity \{40\%; 100\%\}; and emotion \{happy; afraid\} $\times$ scrambling \{6; 10; 14; 18; 22; 26\}) - yielding a total of 160 trials.
		\item The same person's face (even with a different emotion) should not appear more than once in one slide.
		\item Each face picture should appear no more than 3 times in the entire experiment.
	    \end{itemize}
	    To better determine the appropriate response window duration for further trials, we have decided on a conservative duration of \SI{5}{\second}.
	    
	    Due to concerns regarding the recognisability of facial features (as detailed below and in section~\ref{sec:r_pe_ss}) we have decided to test a alternate means of scrambling:
	\subsection{Composite (Kernel-and-Cluster-Based) Scrambling}\label{sec:m_pe_cs}
	    As the previous set-up of preliminary experiments proved unsatisfactory due to eye visibility (which is known to elicit fearful emotional reactions ??? that could well disrupt our brain imaging baseline), we decided to obfuscate facial features.
	    This was done by kernel-scrambling the images (as detailed in section~\ref{sec:m_vs_si_kbs}) before applying the same cluster-based scrambling as in the experiments mentioned above.
	    Such a preparation of the images would render all features with a horizontal and/or vertical spatial frequency above $\frac{1}{\sigma_{K}}$ (where $\sigma_{K}$ is the pixel redistribution standard deviation demonstratively calculated in figure~\ref{eq:lrgn}) unrecognisable.
	    
	    The transition from the black of the pupil over the iris to the white of the cornea in our stimulus images averages around \SI{8}{\pixel} (based on manual analysis).
	    We therefore experimented with a series of kernel-based scrambling steps around that value.
	    
	    We found that for smaller scrambling kernels, the spatial frequency low-pass cut-off was too low (as seen in in figures \ref{fig:m_vs_pe_1_a} and \ref{fig:m_vs_pe_1_b} where the eyes are still visible).
	    We also found that for the \SI{8}{\pixel} scrambling kernel, the subsequent cluster-based scrambling includes very many clusters with predominantly background patches (as seen in figure~\ref{fig:m_vs_pe_1_d})
	    This happens because as the scrambling kernel becomes larger face-pixels diffuse further into the background forming a sparsely populated halo which still has to be included to preserve pixel consistency.
	    
	    For the \SI{6}{\pixel} kernel we found that the eyes were still recognisable in the picture which had not yet undergone cluster-based scrambling, but became unrecognisable after cluster-based scrambling.
	    The recognition of the eyes in the first image of figure~\ref{fig:m_vs_pe_1_c} relies on contextual information from other low-frequency features.
	    As soon as the image is scrambled on a cluster basis, a second high-pass cut-off takes place - 
	    after which only features with a vertical and/or horizontal spatial frequency strictly above $\frac{1}{\SI{26}{\pixel}}$ (\SI{26}{\pixel} being the cluster height and width) can be distinguished.
	    With the contextual information from other features such as brows and nose gone, the pupil, the iris, and adjacent portions of the eyelids only look like a dark blob.
	    Based on these tests we have decided to use faces pre-scrambled with a kernel of \SI{6}{\pixel} for the second round of reaction time testing.
	    
	    \begin{figure}
		\begin{subfigure}[b]{0.495\textwidth}
		    \centering\scalebox{0.3}{\includegraphics{./img/01F_FE_C100_px2rand.jpg} \includegraphics{./img/01F_FE_C100_px2rand_cell26rand.jpg}}
		    \subcaption{\SI{2}{\pixel} scrambling kernel}
		    \label{fig:m_vs_pe_1_a}
		\end{subfigure}
		~%add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[b]{0.495\textwidth}
		    \centering\scalebox{0.3}{\includegraphics{./img/01F_FE_C100_px4rand.jpg} \includegraphics{./img/01F_FE_C100_px4rand_cell26rand.jpg}}
		    \subcaption{\SI{4}{\pixel} scrambling kernel}
		    \label{fig:m_vs_pe_1_b}
		\end{subfigure}
		
		\vspace{0.5cm}%add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[b]{0.495\textwidth}
		    \centering\scalebox{0.3}{\includegraphics{./img/01F_FE_C100_px6rand.jpg} \includegraphics{./img/01F_FE_C100_px6rand_cell26rand.jpg}}
		    \subcaption{\SI{6}{\pixel} scrambling kernel}
		    \label{fig:m_vs_pe_1_c}
		\end{subfigure}
		~%add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[b]{0.495\textwidth}
		    \centering\scalebox{0.3}{\includegraphics{./img/01F_FE_C100_px8rand.jpg} \includegraphics{./img/01F_FE_C100_px8rand_cell26rand.jpg}}
		    \subcaption{\SI{8}{\pixel} scrambling kernel}
		    \label{fig:m_vs_pe_1_d}
		\end{subfigure}
		\caption{Kernel-based scrambling of faces followed by composite (kernel-based and then cluster-based) scrambling of the same faces. 
		The kernels are ascending in size; images were prepared via Pyxrand\cite{pyxrand} (revision \textcolor{lg}{1eb1f7cd65}).}
		\label{fig:m_vs_pe_1}
	    \end{figure}
    
	    We also decided to re-write the stimulus sequence in a more automated and reliable manner.
	    The automation was done via the sequence-calculation software, Pystim\cite{pystim} - a home-brewed Python script written for the purpose of this thesis and openly published on GitHub.
	    The requirements followed and met by the script are:
	    \begin{itemize}
		\item Each emotional picture should be presented once as the template (emotion defining) figure, once as a target, and once as a distractor.
		\item Trial repeat number for each scrambling severity and emotional concentration should be equal
		\item The same person's face (even with a different emotion) should not appear more than once in any one emotion-recognition trial.
	    \end{itemize}
	    The decision to show all emotional images an equal number of times and present each one as the template lead to a total increase the trials count
	    (8 faces $\times$ 2 genders $\times$ 2 emotions $\times$ 2 intensities \textit{plus} 8 faces $\times$ 2 genders $\times$ 2 emotions $\times$ 6 intensities yields 256 trials as opposed to the 160 resulting from the stimulus sequence in section~\ref{sec:m_pe_ss}).
	    To limit fatigue experienced by our participants we have decided to only test 5 scrambling cell steps at a time (reducing the trial number to 224).
	    
	    Additionally, to constrain test duration and relieve fatigue for our participants we have decided to provide a response window of \SI{4}{\second}.
	    This somewhat undershoots the recommended value from section~\ref{sec:r_pe_ss}.
	    In light of the necessity to reduce test duration and in light of acquisition time considerations further detailed in section~\ref{sec:m_fmri}, we have - however - decided that this is a fair trade-off.
    \section{Functional MRI}\label{sec:m_fmri}
	Patterns of neuronal activity are to be extrapolated from functional magnetic resonance imaging (fMRI) data.
    \section{Occulometry}\label{sec:m_om}
	As part of our experiments we have measured occulometric parameters of our participants during behavioural trials performed inside the MRI scanner.
	Occulometric data was captured via a SemsoMotoric Instruments (SMI) “IVIEW X\textsuperscript{\small\texttrademark} MRI-LR” fMRI-compatible eye tracker.
	\subsection{Eye Tracking}\label{sec:m_om_et}
	    More accurately, we are performing “gaze tracking” rather than “eye tracking”, as the system measures the gaze coordinates rather than eye-in-head angles.
	    We use “eye tracking” as a general term for both measurements, keeping with the trend in both industry specifications\cite{Bojko2006} as well as peer-reviewed publications\cite{Kirk2013}.
	    
	    Gaze coordinates are calculated by the firmware provided with our eye tracker, based on the position of the corneal light source reflection relative to the pupil.
	    The light source needed to afford signal for the eye-tracking camera is built-in and shines infra-red light.
	    This avoids triggering the pupillary light reflex \cite{Ellis1981} and distorting the pupil diameter data.
	    As our stimulus presentation does not yet incorporate a drift correction mechanism, our eye-tracking data is subject to gross   
	    
	    The data thus obtained will not - or only tentatively - be described in the “Results” section.
	\subsection{Pupillometry}\label{sec:m_om_pm}
	    From the data provided by the aforementioned hardware we are separately extracting the pupil diameter.
	    The pupil diameter is determined by the firmware of our eye tracker via an edge detection algorithm with a perimeter-to-area ratio constraint
	    (keeping the detected pupil area as close as possible to a circular form).
	    
	    Our data is slightly distorted due to a number of factors - most prominently subjects' eyelids covering parts of the pupils and dark make-up of some female participants being registered as pupil area.
	    These distortions are not critical for gaze point calculation (as long as they remain constant over the course of the measurement); 
	    but they do interfere non-linearly with the firmware-calculated pupil area 
	    (pupil area lost to eyelid coverage, for instance, scales quite complexly with overlap distance - see figure~\ref{eq:cs}).
	    To avoid such complex data-reconstruction algorithms we are using the pupil horizontal diameter as a reference measure for the pupil dilation response in lieu of the actual area.
	    
	    As discussed in section~\ref{sec:r_p} we are considering a number of other data post-processing methods (such as frequency filtering) to eliminate further measurement artefacts from our pupil diameter time course. 
    \section{Statistical Analysis}\label{sec:m_sa}
	For the analysis of our data we have implemented a number of statistic methods from different software packages.
	Our choice of methods took into account transparency and reproducibility (concerns extensively discussed in section~\ref{sec:b_os}), and we have thus - whenever possible - avoided closed-source and/or graphical user interface based software (such as for example MATLAB\textsuperscript{\small\textregistered} or SPSS\textsuperscript{\small\textregistered}).
	Many of the statistical analysis functions are called at compile time from the \LaTeX\ code of this document, meaning that many of our statistical results are live.
	
	For ease of overview we have compiled a short index with explicit naming and descriptions of our statistical analysis tools.
	The methods detailed hereafter will be referred to in further sections of this document simply by their subsection names.
	\subsection{Paired T-Test}\label{sec:m_sa_rs}
	    The t-test is a method used to test the null hypothesis that means of different groups are equal.
	    The most frequently reported resulting measure of the test (the \textit{p}-value) represents the probability of the presented data being observed if the null hypothesis is true. 
	    The t-test makes a small series of assumptions, among which is the independence of data points and the normal distribution of data.
	    
	    Our experiments mainly rely on testing the same population on various conditions.
	    To adapt the t-test to this constraint (where data points are no longer independent) we use the paired sample t-test (also known as a related sample t-tests) - presented in figure~\ref{eq:m_sa_pt}.
	    We implement this statistical method via the \colorbox{vlg}{\texttt{scipy.stats.mstats.ttest\_rel}} function of the SciPy ecosystem \cite{scipy,Oliphant2007}.

	    \begin{figure}[H]
		\[ t = \frac{\sum\limits_{i=1}^n A_{i}-B_{i}}{\sqrt{\frac{n \sum\limits_{i=1}^n (A_{i}-B_{i})^{2} - \left(\sum\limits_{i=1}^n A_{i}-B_{i}\right)^{2}}{n-1}}} \]
		\caption{The general formula for a paired t-test. $A$ and $B$ represent the arrays of data points being tested, and $n$ is the number of data points in either of $A$ and $B$.}
		\label{eq:m_sa_pt}
	    \end{figure}
	    
	    Unless otherwise specified, in order to avoid inflating our significance we are testing per-participant means instead of raw data points.
	    This also affords the advantage of averting undocumented violations of the normal distribution assumption of the t-test -
	    since means of even non-normally distributed populations tend to be normally distributed.  
	\subsection{Standard Error of the Mean}\label{sec:m_sa_se}
	    Confusion around appropriate usage of the standard deviation (SD) and standard error of the mean (SEM) is a significant quality deficit and cause of imprecision in modern research \cite{Nagele2003}.
	    SD (see figure~\ref{eq:m_sa_sd}) is a measure of single data point variability and tends to be constant over increasing sample size, 
	    whereas SEM (see figure~\ref{eq:m_sa_se}) is a measure of the data sample mean reliability and tends to decrease as the sample increases \cite{Altman2005,Streiner1996}.
	    
	    \begin{multicols}{2}
		\begin{figure}[H]
		    \[ \sigma = \sqrt{\frac{1}{n-1} \sum\limits_{i=1}^n (x_i - \bar{x})^2} \]
		    \caption{The general formula for the standard deviation. $n$ represents the number of observations, with $x_i$ being the i-th observation and $\bar{x}$ the mean for all observations.}
		    \label{eq:m_sa_sd}
		\end{figure}
		\begin{figure}[H]
		    \[ SEM = \frac{\sigma}{\sqrt{n}} = \frac{\sqrt{\frac{1}{n-1} \sum\limits_{i=1}^n (x_i - \bar{x})^2}}{\sqrt{n}} \]
		    \caption{The general formula for the standard error of the mean. As indicated in the first form of the formula this is the standard deviation divided by the square root of the number of observations.}
		    \label{eq:m_sa_se}
		\end{figure}
	    \end{multicols}
	    
	    As we are in more cases concerned with per-category means than data point variability, our error bars of choice represent the SEM by default.
	    Cases in which graphical depiction of the SD is helpful in understanding the data at hand are described accordingly and explicitly.
	    
	    In our purpose-built scripts, faceRT\cite{faceRT} and faceOM\cite{faceOM}, we use the \colorbox{vlg}{\texttt{scipy.stats.sem}} and \colorbox{vlg}{\texttt{numpy.std}} functions of the SciPy ecosystem \cite{scipy,Oliphant2007} -
	    for the standard error of the mean and standard deviation respectively.
	\subsection{ANOVA}\label{sec:m_sa_a}
	    ANOVA, the so-called analysis of variance, is a very popular set of statistical methods in psychology and related fields.
	    Formally, ANOVA tests the null hypothesis that the means of defined groups are equal.
	    
	    The method, however, has a series of limitations:
	    \begin{itemize}
		\item ANOVA provides no information as to the group whose mean is outside the range of other groups, nor does it indicate whether this is one group or a number of groups.
		\item ANOVA does not deal well with missing measurements (which motivates scientists to perform ANOVA on means rather than on raw data points - and thus discard valuable information) \cite{Gueorguieva2004}.
		\item The assumptions made by ANOVA - independence of observations, normal distribution of residuals, and homoscedasticity \cite{Anderson1996} - are likely to be violated due to autocorrelation whenever using raw data points. 
	    \end{itemize}
	    Overall, statisticians involved in biology and psychology recommend against ANOVA and point to mixed models (see section~\ref{sec:m_sa_lm}) as a substitute \cite{Baayen2008,Gueorguieva2004}. 
	    In light of these recommendations we shall only be using ANOVA in cases where raw data point sets represent per-participant values 
	    (such as in questionnaires, or in error rates analysis - as seen under section~\ref{sec:r_pe_ss}).
	    
	    In those cases where we do choose to resort to ANOVA, we have to consider that the standard one-way ANOVA assumes all data points are independent.
	    For our applications - where we are testing the same sample of participants over a number of conditions - the appropriate variation of ANOVA would be the repeated measure ANOVA \cite{Gueorguieva2004}.
	    Our ANOVA function of choice is \colorbox{vlg}{\texttt{stats::aov}} \cite{Chambers1992} from the R statistical environment \cite{R}.
	    We are using this function in instances where we compute ANOVA results \textit{live} whenever this document is compiled; for this purpose we are using the interstats\cite{interstats} software package as a wrapper.
	    
	    Additionally, we will be using a pseudo-ANOVA, the “analysis of deviance” \cite{Hastie1992} to test the quality of our linear models. 
	\subsection{Mixed Models}\label{sec:m_sa_lm}
	    Current literature \cite{Baayen2008} recommends mixed models over ANOVA's standard linear model in order to describe the effect of multiple experimental conditions on measurements.
	    
	    We fit mixed models to our data via the \colorbox{vlg}{\texttt{lme4::lmer}} \cite{Bates2005,Bates2007} R function.
	    This function - though one of the most powerful current tools for mixed modelling - fails to provide \textit{p}-values.
	    The author has justified this \cite{Bates2006}, and generally recommended against using \textit{p}-values with such models.
	    In spite of this recommendation, we hold that \textit{p}-values are a stringent standard in quantifying the reliability of scientific observations -
	    and will be using the \colorbox{vlg}{\texttt{nlme::lme}} \cite{Pinheiro2013} function whenever we believe \textit{p}-values are needed.
	    
	    We are using these function in many instances to compute mixed models \textit{live} in this document.
	    For this purpose we are using the interstats \cite{interstats} software package as a wrapper.
