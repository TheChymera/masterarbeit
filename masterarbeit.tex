\input{header.tex}
\chapter{Summary}	
\CatchFileBetweenDelims{\readme}{README.rst}{.. letag}{.. letag>} %adds file content between the specified tags 
\readme
\chapter{Background}
    \section{Pupillometry}\ref{sec:b_p}
    \section{Open Science}\ref{sec:b_os}
\chapter{Methods}
    \section{Questionnaires}
	We have decided to complement our studies with a series of validated and widely used questionnaires.
	These tests could provide valuable insight into the interplay between psychotypes on one side and behavioural parameters or their neurophysiological underpinnings on the other.
	For test presentation we have decided on a web-based approach, for which we have used a web-enabled version of the free and open source (FOSS) software testMaker\cite{testmaker}.
	
	The choice of tests was based on both general relevance to the host group's research, and specific relevance to emotional perception.
	According to these criteria, we have compiled a selection of 8 questionnaires:
	\begin{itemize}
	    \item The Autism Spectrum Quotient questionnaire (\textbf{AQ}) \cite{Baron-Cohen2001}
	    \item The World Health Organization Adult ADHD Self-Report Scale (\textbf{ASRS}) \cite{Kessler2005}
	    \item The 1996 revised Beck Depression Inventory (\textbf{BDI2}) \cite{Beck1996}
	    \item The short version of the Borderline Symptoms List (\textbf{BSL23}) \cite{Bohus2009}
	    \item The Empathizing Quotient questionnaire (\textbf{EQ}) \cite{Baron-Cohen2004}
	    \item The Systemizing Quotient questionnaire (\textbf{SQ}) \cite{Baron-Cohen2003a}
	    \item The Emotion Regulation Questionnaire (\textbf{ERQ}) \cite{Gross2003}
	    \item The Schizotypal Personality Questionnaire (\textbf{SPQ}) \cite{Raine1991}
	\end{itemize}
	The questionnaires were administered in German language, and translated and validated versions were provided by the test databases of the Central Institute of Mental Health.
	Translation references are as follows:
	\begin{itemize}
	    \item \textbf{AQ} - Translated by C.M. Freitag, Homburg
	    \item \textbf{ASRS} - Translated by Dr. P. Kirsch, JLU Gießen 2005
	    \item \textbf{BDI2} - Translated by Pearson Assessment \& Information GmbH (Frankfurt/M. 2010)
	    \item \textbf{BSL23} - Translated at the Department of Psychosomatic Medicine and Psychotherapy, Central Institute of Mental Health.
	    \item \textbf{EQ} - Translated by Dipl.-Psych. Jörn de Haen
	    \item \textbf{SQ} - Translated by Dipl.-Psych. Jörn de Haen
	    \item \textbf{ERQ} - Translated by Brigit Abler and Henrik Kessler \cite{Abler2009}
	    \item \textbf{SPQ} - Tranlsated by Klein, Andersen, and Jahn \cite{Klein1997}
	\end{itemize}
	The questionnaires were evaluated with questioPy\cite{questiopy}, a Python script written for the purpose of this thesis and openly published on GitHub\cite{github}.
    \section{Behavioural Test}\label{sec:m_bt}
	In our current study we are exploring neuronal function ??ref in visual emotional perception.
	Our behavioural test of choice to tap into these systems relies on visual recognition and subsequent matching of emotion.
	We call our paradigm a Hariri-style behavioural test, in acknowledgement of the seminal implementations of a similar paradigm by Hariri and colleagues \cite{Hariri2000,Hariri2003}.
	    
	The paradigm consists of a triangular arrangement of three faces - with the bottom two aligned horizontally and the upper face displaced vertically and equidistant from both bottom faces.
	The behavioural test prompts the participant to observe a certain feature (in our case emotion, or - as described in section~\ref{sec:m_vs_si} - pixel arrangement) of the top face, and to select of the bottom faces the one repeating that feature.
	
	In our case bottom face selection was done via the arrow keys of a keyboard (in the preliminary experiments further described under section~\ref{sec:m_pe}) and via a remote control (in the fMRI trials further described under section~\ref{sec:m_fmri}).
	The participants received no sort of feedback after reacting to the individual items, and the images were continuously displayed independently of input for \SI{5}{\second} or \SI{4.5}{\second} (in the preliminary experiments), and for \SI{4}{\second} (in the fMRI trials); intertrial intervals showed a point or a cross and lasted for \SI{2}{\second} or approximately \SI{4}{\second} respectively.
	The higher inter-trial interval duration for the fMRI experiments was chosen to accomodate for two volume acquisitions, and we say “approximately” because a jitter was added (more on that in section~\ref{sec:m_fmri}). 
	
	During the above we take measures for valence (correct or incorrect) of responses, and for reaction times.
	
	\begin{wrapfigure}{r}{0.45\textwidth}
	  \centering
	    \scalebox{0.55}{\includegraphics{./img/vis_field.jpg}}
	    \caption{Perimetric map of the human field of view \cite{Ruch1960}.
	    For measurement the head and eyes were fixed, with the fovea pointing at \SI{0}{\degree} on the cross-hairs.
	    The white area affords binocular vision, the black area is completely outside the field of view.}
	    \label{fig:m_b_1}
	    \vspace{-1.0cm}
	\end{wrapfigure}
	
	In contrast to other formulations\cite{Hariri2000,Hariri2003} of the same paradigm (which keep all 3 stimuli equidistant),
	we have decided to adapt the positions of stimulus images to what we believe is a better fit for the human field of view.
	Simply aligning the midpoints of stimulus images to the vertices of an equilateral triangle presents some issues:
	\begin{itemize}
	    \item Not accounting for the aspect ratio of the actual stimulus images lessens control over the resulting populated visual window (pVW).
	    \item Horizontal and diagonal distances are not perceived identically by human vision 
	\end{itemize}
	
	We tackled the above by adapting the pVW to the normal human field of vision aspect ratio.
	To do so, we took the outermost non-neutral pixels (rather than the centers of the stimulus images) and used these as pVW delimiters in deciding where to position our images.
	
	For the following distance specifications we have defined the unit “u” as the height of our stimulus images.
	Seeing as our stimulus images have a fixed aspect ratio their width is thus consistently \SI[parse-numbers = false]{\frac{3}{4}}{u}.
	
	The static human field of view has a complex shape modulated amongst others by facial features (as seen in figure~\ref{fig:m_b_1}).
	If the outermost light paths fitting figure~\ref{fig:m_b_1} were to be circumscribed by a planar mask in the shape of the rectangle, its height would span approximately \SI{130}{\degree} and its width \SI{180}{\degree}.
	This gives the rectangular fit for the field of view a width-to-height ratio of 0.72(2).
	Given the data - and for numerical convenience - we have chosen to position the midpoints of our stimulus images as follows: 
	the distance between the two lower image midpoints is \SI{2}{u}; and the vertical offset of the top image midpoint from the horizontal line connecting the bottom image midpoints is \SI{1}{u}.
	This gives our pVW a total height of \SI{2}{u} and a total width of \SI[parse-numbers = false]{\frac{11}{4}}{u}.
	The resulting width-to-height ratio of 0.(72) was deemed a sufficiently close match for the afore mentioned human field of view (as approximated by a rectangle).
	We thus have reason to believe that our stimulus item distribution is superior to the one implemented by Hariri and colleagues, and we encourage further users of Hariri-style paradigms to note our observations.  
	
	We are aware that our simplified approximation still does not account for vertical saccades being slower\cite{TerryBahill1975} and less accurate\cite{Collewijn1988} than horizontal saccades, 
	nor for the fact that upward saccades tend to undershoot while downward saccades tend to overshoot \cite{Collewijn1988};
	however, integrating accurate metrics for these kinetics and determining their relevance for Hariri-style paradigms differs in focus from and far exceeds the scope of our current study.
	
	Our resulting pVW was scaled to fit the available display field of the monitor and tilted mirror set-up in our MRI scanner (more on this in section~\ref{sec:m_om_et}).
	The dimensions (specified in degrees of visual angle) are proportional to the ones mentioned above, scaled for u = \SI{4.24}{\degree}.
	The midpoint of the pVW (at \SI{0.5}{u} downward from the midpoint of the top image) was aligned to the midpoint of the monitor.
	A picture of this visual set-up can be seen in figure~\ref{fig:m_b_2}.
	
	\begin{figure}[!h]
	    \centering{\includegraphics[width=1\textwidth]{./img/01F_FE_C100_em100_composite.jpg}}
	    \caption{Sample image from the emotional face matching trials in our Hariri-style behavioural test.}
	    \label{fig:m_b_2}
	\end{figure}
		    
    \section{Visual Stimuli}
	\subsection{Emotional Faces}\label{sec:m_vs_ef}
	    The emotional face stimuli required by our behavioural test paradigm (see section~\ref{sec:m_bt}) have been sourced from the NimStim set of facial expressions \cite{Tottenham2009}.
	\subsection{Scrambled Images}\label{sec:m_vs_si}
	    In order to define a baseline of visual recognition we have decided to use trials in which visual input consists of the same pixels as in the emotional trials. 
	    To disentangle visual from semantic (emotion) identification we decided to use “scrambled” images - in which pixels from the emotional faces are permuted so as to obfuscate emotional expressions and facial features.
	    The recognition trials would thus entail matching identical permutations of the same picture.
	    For these trials the template and target will be the exact same image and the distractor will be a different permutation of the pixels which constitute the image the template and target were computed from. 
	    
	    There will be two sets of scrambled image trials: “easy” and “difficult” - to act as baselines for easy and difficult emotion recognition trials respectively.
	    Recognition difficulty of the scrambled image trials should scale proportionally with that of the easy and difficult emotion recognition trials (detailed under section~\ref{sec:m_vs_ef}).
	    To achieve this, scrambling has to be progressively complex.
	    Available data ??? suggests that scrambling of images via smaller sub-sections (further referred to as “clusters”) makes the images progressively difficult to identify and match to other copies.
	    This rationale determined the nature of our scrambling algorithms (detailed below) and was also experimentally validated in preliminary trials (as described in section~\ref{sec:r_pe}).
	    
	    Based on the preliminary results detailed in section~\ref{sec:r_pe} we have decided to use composite (kernel \textit{and} cluster based) scrambling for our experiment.
	    The scrambling kernel was constant at \SI{6}{\pixel} (as explained in section~\ref{sec:m_pe_cs}) 
	    and “hard” recognition trials had a scrambling cluster size of \SI{10}{\pixel} while “easy” trials had a scrambling cluster of \SI{22}{\pixel}.
	    
	    Scrambling was done via Pyxrand\cite{pyxrand}, home-brewed Python script written for the purpose of this thesis and openly published on GitHub.
	    The script provides both cluster-based and kernel-based scrambling:
	    \subsubsection{Cluster-Based Scrambling}
		This scrambling functionality recognises the face region of interest (ROI) by scanning for pixel lines with few unique values, and then divides the face ROI in square clusters of predefined sizes.
		The clusters then get permuted and rewritten in-place on the image - this is done via the \colorbox{vlg}{\texttt{montage2d}} function of the \colorbox{vlg}{\texttt{scikits\_image}} python package 
		(the function was written and contributed to the package for the benefit of the scientific community as part of this thesis).
		The image background is then filled with homogeneous values.
	    \subsubsection{Kernel-Based Scrambling}\label{sec:m_vs_si_kbs}
		This is done by remapping single pixels via the \colorbox{vlg}{\texttt{geometric\_transform}} function of the \colorbox{vlg}{\texttt{scipy}} Python suite. 
		New positions are computed via a function which adds a random integer in the $[-K;K]$ ($K$ being the scrambling kernel integer) interval to both the X and Y coordinates of the said pixel.
		Effectively, this redistributes each pixel in an area of $[-K;K]$ around its original position with a standard deviation ($\sigma$) of $\approx 0.96K$ (as calculated in figure~\ref{eq:lrgn}).
    \section{Preliminary Experiments}\label{sec:m_pe}
	Preliminary experiments have been conducted in order to establish a proper paradigm for the main experiments of the project. 
	Their primary goal is determining which scrambling cluster sizes have reaction times comparable to the \SI{100}{\percent} and \SI{40}{\percent} emotional images (decided upon based on rationales delineated under section~\ref{sec:m_vs_ef}).
	
	For stimulus presentation and data acquisition in these experiments we used faceRT\cite{faceRT}, a home-brewed Python script written for the purpose of this thesis and openly published on GitHub.
	Our Python script uses the PsychoPy suite\cite{Peirce2008} for specialized, high-precision stimulus rendering and timing.
	To provide equal sample sizes (assumed by a number of statistical methods listed under section~\ref{sec:m_sa}) trials with no response present have been assigned a penalty reaction time of \SI{5}{\second}.
	
	Participants were recruited from staff at the Central Institute of Mental Health, and students of the Universities of Mannheim and Heidelberg.
	Since these are preparatory trials we opted for merely around 7 participants per run.
	Participants are identified in the study by a 2-letter code, and a subset of them have participated in multiple runs of our preliminary trials.
	It should be noted that runs are therefore not well suited for direct comparison or for binning; but they do offer a good predictor for expected variability in reaction times.
	
	For the following categories of experiments we have adhered to the behavioural test specifications under section~\ref{sec:m_bt}.  
	\subsection{Simple Cluster-Based Scrambling}\label{sec:m_pe_ss}
	    In a first set of trials we have scrambled images with cluster sizes starting at \SI{6}{\pixel} and progressing to \SI{26}{\pixel} in \SI{4}{\pixel} steps.
	    The stimulus presentation protocol used for these trials is revision \textcolor{lg}{ce2b3a8f30024504d7f7b087211fdbb8b5e47c3e} of the faceRT\cite{faceRT} script suite.
	    Stimuli were presented fully randomized per-participant, and sequences were compiled manually, following the listed requirements:
	    \begin{itemize}
		\item Each condition should have 10 corresponding trials (conditions are: emotion \{happy; afraid\} $\times$ emotion intensity \{40\%; 100\%\}; and emotion \{happy; afraid\} $\times$ scrambling \{6; 10; 14; 18; 22; 26\}) - yielding a total of 160 trials.
		\item The same person's face (even with a different emotion) should not appear more than once in one slide.
		\item Each face picture should appear no more than 3 times in the entire experiment.
	    \end{itemize}
	    To better determine the appropriate response window duration for further trials, we have decided on a conservative duration of \SI{5}{\second}.
	    
	    Due to concerns regarding the recognisability of facial features (as detailed below and in section~\ref{sec:r_pe_ss}) we have decided to test a alternate means of scrambling:
	\subsection{Composite (Kernel-and-Cluster-Based) Scrambling}\label{sec:m_pe_cs}
	    As the previous set-up of preliminary experiments proved unsatisfactory due to eye visibility (which is known to elicit fearful emotional reactions ??? that could well disrupt our brain imaging baseline), we decided to obfuscate facial features.
	    This was done by kernel-scrambling the images (as detailed in section~\ref{sec:m_vs_si_kbs}) before applying the same cluster-based scrambling as in the experiments mentioned above.
	    Such a preparation of the images would render all features with a spatial frequency above $\frac{1}{\sigma_{K}}$ (where $\sigma_{K}$ is the pixel redistribution standard deviation deonstratively calculated in figure~\ref{eq:lrgn}) unrecognisable.
	    
	    The transition from the black of the pupil over the iris to the white of the cornea in our stimulus images averages around \SI{8}{\pixel} (based on manual analysis).
	    We therefore experimented with a series of kernel-based scrambling steps around that value.
	    
	    We found that for smaller scrambling kernels, the spatial frequency low-pass cut-off was too low (as seen in in figures \ref{fig:m_vs_pe_1_a} and \ref{fig:m_vs_pe_1_b} where the eyes are still visible).
	    We also found that for the \SI{8}{\pixel} scrambling kernel, the subsequent cluster-based scrambling includes very many clusters with predominantly background patches (as seen in figure~\ref{fig:m_vs_pe_1_d})
	    This happens because as the scrambling kernel becomes larger face-pixels diffuse further into the background forming a sparsely populated halo which still has to be included to preserve pixel consistency.
	    
	    For the \SI{6}{\pixel} kernel we found that the eyes were still recognisable in the picture which had not yet undergone cluster-based scrambling, but became unrecognisable after cluster-based scrambling.
	    The recognition of the eyes in the first image of figure~\ref{fig:m_vs_pe_1_c} relies on contextual information from other low-frequency features.
	    As soon as the image is scrambled on a cluster basis, a second high-pass cut-off takes place - 
	    after which only features with a vertical or horizontal spatial frequency strictly above $\frac{1}{\SI{26}{\pixel}}$ (\SI{26}{\pixel} being the cluster height and width) can be distinguished.
	    With the contextual information from other features such as brows and nose gone, the pupil, the iris, and adjacent portions of the eyelids only look like a dark blob.
	    Based on these tests we have decided to use faces pre-scrambled with a kernel of \SI{6}{\pixel} for the second round of reaction time testing.
	    
	    \begin{figure}
		\begin{subfigure}[b]{0.495\textwidth}
		    \centering\scalebox{0.3}{\includegraphics{./img/01F_FE_C100_px2rand.jpg} \includegraphics{./img/01F_FE_C100_px2rand_cell26rand.jpg}}
		    \subcaption{\SI{2}{\pixel} scrambling kernel}
		    \label{fig:m_vs_pe_1_a}
		\end{subfigure}
		~%add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[b]{0.495\textwidth}
		    \centering\scalebox{0.3}{\includegraphics{./img/01F_FE_C100_px4rand.jpg} \includegraphics{./img/01F_FE_C100_px4rand_cell26rand.jpg}}
		    \subcaption{\SI{4}{\pixel} scrambling kernel}
		    \label{fig:m_vs_pe_1_b}
		\end{subfigure}
		
		\vspace{0.5cm}%add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[b]{0.495\textwidth}
		    \centering\scalebox{0.3}{\includegraphics{./img/01F_FE_C100_px6rand.jpg} \includegraphics{./img/01F_FE_C100_px6rand_cell26rand.jpg}}
		    \subcaption{\SI{6}{\pixel} scrambling kernel}
		    \label{fig:m_vs_pe_1_c}
		\end{subfigure}
		~%add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[b]{0.495\textwidth}
		    \centering\scalebox{0.3}{\includegraphics{./img/01F_FE_C100_px8rand.jpg} \includegraphics{./img/01F_FE_C100_px8rand_cell26rand.jpg}}
		    \subcaption{\SI{8}{\pixel} scrambling kernel}
		    \label{fig:m_vs_pe_1_d}
		\end{subfigure}
		\caption{Kernel-based scrambling of faces followed by composite (kernel-based and then cluster-based) scrambling of the same faces. 
		The kernels are ascending in size; images were prepared via Pyxrand\cite{pyxrand} (revision \textcolor{lg}{1eb1f7cd65e4fc43b3454728ffeac3dedbcc4312}).}
		\label{fig:m_vs_pe_1}
	    \end{figure}
    
	    We also decided to re-write the stimulus sequence in a more automated and reliable manner.
	    The automation was done via the sequence-calculation software, Pystim\cite{pystim} - a home-brewed Python script written for the purpose of this thesis and openly published on GitHub.
	    The requirements followed and met by the script are:
	    \begin{itemize}
		\item Each emotional picture should be presented once as the template (emotion defining) figure, once as a target, and once as a distractor.
		\item Trial repeat number for each scrambling severity and emotional concentration should be equal
		\item The same person's face (even with a different emotion) should not appear more than once in any one emotion-recognition trial.
	    \end{itemize}
	    The decision to show all emotional images an equal number of times and present each one as the template lead to a total increase the trials count
	    (8 faces $\times$ 2 genders $\times$ 2 emotions $\times$ 2 intensities \textit{plus} 8 faces $\times$ 2 genders $\times$ 2 emotions $\times$ 6 intensities yields 256 trials as opposed to the 160 resulting from the stimulus sequence in section~\ref{sec:m_pe_ss}).
	    To limit fatigue experienced by our participants we have decided to only test 5 scrambling cell steps at a time (reducing the trial number to 224).
	    
	    Additionally, to constrain test duration and relieve fatigue for our participants we have decided to provide a response window of \SI{4}{\second}.
	    This somewhat undershoots the recommended value from section~\ref{sec:r_pe_ss}.
	    In light of the necessity to reduce test duration and in light of acquisition time considerations further detailed in section~\ref{sec:m_fmri}, we have - however - decided that this is a fair trade-off.
    \section{Functional MRI}\label{sec:m_fmri}
	Patterns of neuronal activity are to be extrapolated from functional magnetic resonance imaging (fMRI) data.
    \section{Occulometry}\label{sec:m_om}
	\subsection{Eye Tracking}\label{sec:m_om_et}
	\subsection{Pupillometry}\label{sec:m_om_pm}
    \section{Statistical Analysis}\label{sec:m_sa}
	For the analysis of our data we have implemented a number of statistic methods from different software packages.
	Our choice of methods took into account transparency and reproducibility (concerns extensively discussed in section~\ref{sec:b_os}), and we have thus - whenever possible - avoided closed-source and/or graphical user interface based software (such as for example MATLAB\textsuperscript{\small\textregistered} or SPSS\textsuperscript{\small\textregistered}).
	Many of the statistical analysis functions are called at compile time from the \LaTeX\ code of this document, meaning that many of our statistical results are live.
	
	For ease of overview we have compiled a short index with explicit naming and descriptions of our statistical analysis tools.
	The methods detailed hereafter will be referred to in further sections of this document simply by their subsection names.
	\subsection{Paired T-Test}\label{sec:m_sa_rs}
	    We are using a paired t-test (also known as a related sample t-tests) to test the null hypothesis that population means for different conditions are equal.
	    \begin{figure}[H]
		\[ t = \frac{\sum\limits_{i=1}^n A_{i}-B_{i}}{\sqrt{\frac{n \sum\limits_{i=1}^n (A_{i}-B_{i})^{2} - \left(\sum\limits_{i=1}^n A_{i}-B_{i}\right)^{2}}{n-1}}} \]
		\caption{The general formula for a paired t-test. $A$ and $B$ represent the arrays of data points being tested, and $n$ is the number of data points in both $A$ and $B$.}
		\label{eq:m_sa_pt}
	    \end{figure}
	\subsection{Standard Error of the Mean}\label{sec:m_sa_se}
	    Confusion around appropriate usage of the standard deviation (SD) and standard error of the mean (SEM) is a significant quality deficit and cause of imprecision in modern research \cite{Nagele2003}.
	    SD (see figure~\ref{eq:m_sa_sd}) is a measure of single data point variability which tends to be constant over increasing sample size, 
	    whereas SEM (see figure~\ref{eq:m_sa_se}) is a measure of the data sample mean reliability and tends to decrease as the sample increases \cite{Altman2005,Streiner1996}.
	    
	    \begin{multicols}{2}
		\begin{figure}[H]
		    \[ \sigma = \sqrt{\frac{1}{n-1} \sum\limits_{i=1}^n (x_i - \bar{x})^2} \]
		    \caption{The general formula for the standard deviation. $n$ represents the number of observations, with $x_i$ being the i-th observation and $\bar{x}$ the mean for all observations.}
		    \label{eq:m_sa_sd}
		\end{figure}
		\begin{figure}[H]
		    \[ SEM = \frac{\sigma}{\sqrt{n}} = \frac{\sqrt{\frac{1}{n-1} \sum\limits_{i=1}^n (x_i - \bar{x})^2}}{\sqrt{n}} \]
		    \caption{The general formula for the standard error of the mean. As indicated in the first form of the formula this is the standard deviation divided by the square root of the number of observations.}
		    \label{eq:m_sa_se}
		\end{figure}
	    \end{multicols}
	    
	    As we are more concerned with per-category means than data point variability our error bars of choice represent the SEM.
	    Cases in which graphical depiction of the SD is helpful in understanding the data at hand are accordingly and explicitly detailed.  
	    
\chapter{Results}                                                                          
    \section{Preliminary Experiments}\label{sec:r_pe}
	In order to establish an experimental paradigm which affords the comparison between emotion recognition and simple visual matching, we need to select stimuli whose matching is correspondingly difficult.
	For the emotion recognition trials, we decided on faces with emotional concentrations of 40\% and 100\% (as discussed in section~\ref{sec:m_vs_ef}).
	
	In the following experiments we examine reaction times and error rates for the recognition of progressively scrambled images in comparison to the recognition of our two emotional concentrations.
	
	The reaction time figures which we present include both per-participant and population sample bar plots.
	As our sample sizes are reduced, we acknowledge the limited reliability with which we can make claims pertaining to the general population. 
	Instead, we choose to provide a description of mean reaction time reliability for the tested populations.
	
	
	As exemplified in figure~\ref{fig:r_pe1}, both inter-trial and inter-participant variability is high
	\subsection{Simple Cluster-Based Scrambling}\label{sec:r_pe_ss}
	    For the results presented in figure~\ref{fig:r_pe_ss1} we have used a simple cluster-based scrambling algorithm and a hand-written balanced stimulus sequence 
	    (both detailed in section~\ref{sec:m_pe_ss}).
	    \py{fig_ss1}
	    
	    As can be seen in figure~\ref{fig:r_pe_ss1} the difficult emotion recognition condition requires a similar amount of time for completion as recognition of the \SI{6}{\pixel} scrambled images 
	    (with two-tailed \textit{p} = \py{"{:.1e}".format(ttest_rel(data_ss1[(data_ss1['scrambling'] == 0) & (data_ss1['intensity'] == 40)]['RT'], data_ss1[(data_ss1['scrambling'] == 6)]['RT'])[1])}).
	    With increasing scrambling cluster size, reaction times are reduced - reaching approximately the same values as for easy emotion recognition already at cluster size of \SI{14}{\pixel}
	    (two-tailed \textit{p} = \py{"{:.1e}".format(ttest_rel(data_ss1[(data_ss1['scrambling'] == 0) & (data_ss1['intensity'] == 100)]['RT'], data_ss1[(data_ss1['scrambling'] == 14)]['RT'])[1])}).
	    For increasing cluster sizes the reaction times plateau. 
	    The two-tailed \textit{p}-values for \SI{18}{\pixel}, \SI{22}{\pixel}, and \SI{26}{\pixel} cluster sizes compared to easy emotion recognition are:
	    \py{"{:.1e}".format(ttest_rel(data_ss1[(data_ss1['scrambling'] == 0) & (data_ss1['intensity'] == 100)]['RT'], data_ss1[(data_ss1['scrambling'] == 18)]['RT'])[1])}, \py{"{:.1e}".format(ttest_rel(data_ss1[(data_ss1['scrambling'] == 0) & (data_ss1['intensity'] == 100)]['RT'], data_ss1[(data_ss1['scrambling'] == 22)]['RT'])[1])}, and \py{"{:.1e}".format(ttest_rel(data_ss1[(data_ss1['scrambling'] == 0) & (data_ss1['intensity'] == 100)]['RT'], data_ss1[(data_ss1['scrambling'] == 26)]['RT'])[1])} respectively.
	    
	    The abrupt decrease of the reaction times over increasing cluster sizes raised some suspicion.
	    The plateau suggested that scrambling cluster sizes of \SI{14}{\pixel}, \SI{18}{\pixel}, \SI{22}{\pixel}, and \SI{26}{\pixel} possessed a recognition-relevant quality which the \SI{6}{\pixel} scrambling cluster did not.
	    If this purported feature is to explain the reaction time distribution, it should emerge at a scrambling cluster size of around \SI{10}{\pixel}.
	    Visual scrutiny of the images suggested that the emergent feature may be recognizable facial features - more precisely the eyes (with pupil and iris having a diameter of approximately \SI{8}{\pixel}).
	    To test our hypothesis and define a more appropriate baseline for simple visual recognition, we devised a new set of experiments - described in section~\ref{sec:m_pe_cs}, and analyzed in section \ref{sec:r_pe_cs}.
	    
	    In addition to the reaction time per-category analysis, we attempted to gauge the maximal window for reaction times which we should provide in further experiments.
	    For this analysis we have selectively looked at reaction times for images processed with \SI{0}{\pixel}, \SI{6}{\pixel}, and \SI{14}{\pixel} scrambling clusters.
	    We have based this restriction on our categories of interest 
	    (\SI{0}{\pixel} scrambling is equivalent to the emotion recognition trials, \SI{6}{\pixel} scrambling contains simple visual recognition trials not significantly different from hard emotion recognition, and \SI{14}{\pixel} scrambling contains the first set of trials not significantly different from easy emotion recognition).
	    \py{fig_ss2}
	    
	    Figure~\ref{fig:r_pe_ss2} shows the reaction time distribution for the said subset of trials.
	    The mean value ($\mu$) for the reaction times is approximately \pySI{np.around(data_ss2['RT'].mean(), decimals=2)}{\second}, 
	    and the standard deviation ($\sigma$) of the fitted normal distribution is approximately \pySI{np.around(np.std(data_ss2['RT']), decimals=2)}{\second}.
	    Based on the three-sigma rule (see figure~\ref{eq:3s} for a general expression) we have decided that a reaction time cut-off of \pySI{np.around(data_ss2['RT'].mean()+3*np.std(data_ss2['RT']), decimals=2)}{\second} should not statistically distort our data.
	    
	    We have also tried to analyse the error rates in our Hariri-style matching task in order to extract more information on the appropriateness of our baseline.
	    The results herefor are presented in figure~\ref{fig:r_pe_ss3}.
	    \py{fig_ss3}
	    
	    Our error rate distribution over categories proves difficult to interpret.
	    It is obvious for one thing that error rates for most categories vary greatly among participants.
	    The main conclusion which can be drawn based upon these data is that emotion recognition in easy trials is probably more robust than image matching in all other categories.
	    One tailed \textit{p} tests for the aforementioned assumption approach significance for all comparisons; 
	    with \textit{p} values of 
	    \py{"{:.1e}".format(ttest_rel(data_ss3[(data_ss3['scrambling'] == 0) & (data_ss3['intensity'] == 100)]['error rate'], data_ss3[(data_ss3['scrambling'] == 0) & (data_ss3['intensity'] == 40)]['error rate'])[1]/2)},
	    \py{"{:.1e}".format(ttest_rel(data_ss3[(data_ss3['scrambling'] == 0) & (data_ss3['intensity'] == 100)]['error rate'], data_ss3[(data_ss3['scrambling'] == 6)]['error rate'])[1]/2)},
	    \py{"{:.1e}".format(ttest_rel(data_ss3[(data_ss3['scrambling'] == 0) & (data_ss3['intensity'] == 100)]['error rate'], data_ss3[(data_ss3['scrambling'] == 10)]['error rate'])[1]/2)},
	    \py{"{:.1e}".format(ttest_rel(data_ss3[(data_ss3['scrambling'] == 0) & (data_ss3['intensity'] == 100)]['error rate'], data_ss3[(data_ss3['scrambling'] == 14)]['error rate'])[1]/2)},
	    \py{"{:.1e}".format(ttest_rel(data_ss3[(data_ss3['scrambling'] == 0) & (data_ss3['intensity'] == 100)]['error rate'], data_ss3[(data_ss3['scrambling'] == 18)]['error rate'])[1]/2)},
	    \py{"{:.1e}".format(ttest_rel(data_ss3[(data_ss3['scrambling'] == 0) & (data_ss3['intensity'] == 100)]['error rate'], data_ss3[(data_ss3['scrambling'] == 22)]['error rate'])[1]/2)}, and
	    \py{"{:.1e}".format(ttest_rel(data_ss3[(data_ss3['scrambling'] == 0) & (data_ss3['intensity'] == 100)]['error rate'], data_ss3[(data_ss3['scrambling'] == 26)]['error rate'])[1]/2)}
	    for comparisons of easy emotion recognition with hard emotion recognition, and simple visual recognition of \SI{6}{\pixel}, \SI{10}{\pixel}, \SI{14}{\pixel}, \SI{18}{\pixel}, \SI{22}{\pixel}, and \SI{26}{\pixel} scrambled images respectively.
	    
	    Even for comparable reaction times (easy emotion recognition vs. simple visual recognition of pictures scrambled with \SI{14}{\pixel}, \SI{18}{\pixel}, \SI{22}{\pixel}, and \SI{26}{\pixel} clusters), simple visual recognition is prone to higher error rates than easy emotion recognition.
	    This could raise doubts about our choice of baseline, and should be kept in mind for further scrutiny (detailed in the discussion of figure~\ref{fig:r_pe_cs5}).
	\subsection{Composite (Kernel-and-Cluster-Based) Scrambling}\label{sec:r_pe_cs}
	    Based on the results detailed in section ~\ref{sec:r_pe_ss} we have performed additional experiments using a series of improved method choices (detailed in section \ref{sec:m_pe_cs}).
	    \py{fig_cs1}
	    
	    Figure~\ref{fig:r_pe_cs1} presents the the data from the first run of these experiments.
	    An obvious feature here is the disappearance of the plateau phase, which supports our hypothesis regarding its emergence due to facial features with a spatial frequency of around \SI{8}{\pixel} (formulated in section~\ref{sec:r_pe_ss}).
	    The equivalence of reaction times for images scrambled with \SI{11}{\pixel} and \SI{15}{\pixel} clusters (two-tailed \textit{p} of \py{"{:.1e}".format(ttest_rel(data_cs1[(data_cs1['scrambling'] == 11)]['RT'], data_cs1[(data_cs1['scrambling'] == 15)]['RT'])[1])}) raises concern, not least of all because in light of these results we have identified no appropriate baseline condition for easy emotion recognition.
	    \py{fig_cs2}
	    
	    To further investigate cateory-dependent reaction times in this new condition (and possibly identify a baseline) we have reiterated the experiment with different scrambling cluster sizes.
	    We present the new data in figure~\ref{fig:r_pe_cs2}.
	    The results from this re-run are curious in that they mostly do not corroborate our findings from the previous run (figure~\ref{fig:r_pe_cs1}).
	    The data does seem to indicate an appropriate baseline for easy emotion recognition (namely visual recognition of pictures processed with a scrambling kernel of \SI{6}{\pixel} and a scrambling cluster of \SI{10}{\pixel}: two-tailed \textit{p} = \py{"{:.1e}".format(ttest_rel(data_cs2[(data_cs2['scrambling'] == 0) & (data_cs2['intensity'] == 100)]['RT'], data_cs2[(data_cs2['scrambling'] == 10)]['RT'])[1])}).
	    The validity of this finding is however undermined by the fact that an only slightly larger (by \SI{1}{\pixel}) scrambling cluster prompted a significantly different reaction time from the difficult emotion recognition in our previous run of the experiment.
	    The emergence of a plateau of reaction times for larger scrambling clusters (previously observed in section~\ref{sec:r_pe_ss}) raises further questions.
	    
	    We believe that this variability is due to our reduced and varying participant sample, and that this merits further investigation.
	    Concomitantly to this realization we have also decided to adapt the size of the visual window to accurately match the degrees of visual angle which our participants would be seeing in our fMRI-monitor set-up. This is a slight decrease in size from the trials above: u = \SI{4.24}{\degree} now versus u $\approx$ \SI{6}{\degree} earlier (with “u” denoting face image height).
	    
	    This third run of our experiment (with results depicted in figure~\ref{fig:r_pe_cs3}) should be the best approximation for the visual input the participants will be receiving in the fMRI trials.
	    We therefore trust this run above all others as a guideline for further experimental design choices.
	    \py{fig_cs3}
	    
	    Figure~\ref{fig:r_pe_cs3} shows a linear decay of reaction times over increasing scrambling cluster size
	    (as opposed to a plateau for larger scrambling clusters, which he have noticed in some of the previous runs).
	    The data recommends pictures scrambled with a kernel of \SI{6}{\pixel} and a cluster of \SI{11}{\pixel} as an appropriate baseline for hard emotion recognition
	    (2-tailed \textit{p} of \py{"{:.1e}".format(ttest_rel(data_cs3[(data_cs3['scrambling'] == 0) & (data_cs3['intensity'] == 40)]['RT'], data_cs3[(data_cs3['scrambling'] == 11)]['RT'])[1])})
	    and pictures scrambled with a kernel of \SI{6}{\pixel} and a cluster of \SI{23}{\pixel} as an appropriate baseline for easy emotion recognition
	    (2-tailed \textit{p} of \py{"{:.1e}".format(ttest_rel(data_cs3[(data_cs3['scrambling'] == 0) & (data_cs3['intensity'] == 100)]['RT'], data_cs3[(data_cs3['scrambling'] == 23)]['RT'])[1])}).
	    It should be noted that though not significantly different, both of these simple visual recognition reaction times undershoot their counterpart emotion recognition reaction times.
	    In light of this we have decided to use a \SI{6}{\pixel} kernel- and \SI{10}{\pixel} cluster-scrambled image baseline for difficult emotion recognition, 
	    and a \SI{6}{\pixel} kernel- and \SI{22}{\pixel} cluster-scrambled image baseline for easy emotion recognition.
	    \py{fig_cs4}
	    
	    We further decided to analyse whether our slightly over-cropped reaction time window seems to distort our data.
	    In section~\ref{sec:m_pe_cs} we have detailed our decision to settle for a \SI{4}{\second} reaction time window, the predicted ideal cropping being \pySI{np.around(data_ss2['RT'].mean()+3*np.std(data_ss2['RT']), decimals=2)}{\second}, as calculated in section~\ref{sec:r_pe_ss}.
	    In figure~\ref{fig:r_pe_cs4} we see that reaction times seem to fade approximately \SI{0.5}{\second} before our cut-off, and the fitted normal distribution does not indicate expected results beyond that point.
	    We take these results as confirmation that our selected cut-off time is not over-stringent for the selected conditions (easy and hard emotion recognition, plus visual recognition of scrambled images processed with a \SI{6}{\pixel} kernel and \SI{11}{\pixel} and \SI{23}{\pixel} cluster respectively.	    
	    \py{fig_cs5}
	    
	    To re-examine our doubts on the suitability of our baseline concept (raised in section~\ref{sec:r_pe_ss}), we have also examined error rates for this last and methodologically most faithful preliminary experiment run.
	    Figure~\ref{fig:r_pe_cs5} presents a slightly divergent picture from what we have seen in figure~\ref{fig:r_pe_ss3}.
	    For one thing, the easy emotion recognition condition no longer has a null error rate.
	    This removes our concern about this condition prompting a different reliability of recognition.
	    On the other hand, a different condition - this time \textit{difficult} emotion recognition - prompts significantly higher error rates than all the others
	    (one-tailed \textit{p} tests for difficult emotional recognition versus easy emotional recognition and visual recognition with ascending scrambling steps give \textit{p} equal to:
	    \py{"{:.1e}".format(ttest_rel(data_cs5[(data_cs5['scrambling'] == 0) & (data_cs5['intensity'] == 40)]['error rate'], data_cs5[(data_cs5['scrambling'] == 0) & (data_cs5['intensity'] == 100)]['error rate'])[1]/2)},
	    \py{"{:.1e}".format(ttest_rel(data_cs5[(data_cs5['scrambling'] == 0) & (data_cs5['intensity'] == 40)]['error rate'], data_cs5[(data_cs5['scrambling'] == 11)]['error rate'])[1]/2)},
	    \py{"{:.1e}".format(ttest_rel(data_cs5[(data_cs5['scrambling'] == 0) & (data_cs5['intensity'] == 40)]['error rate'], data_cs5[(data_cs5['scrambling'] == 15)]['error rate'])[1]/2)},
	    \py{"{:.1e}".format(ttest_rel(data_cs5[(data_cs5['scrambling'] == 0) & (data_cs5['intensity'] == 40)]['error rate'], data_cs5[(data_cs5['scrambling'] == 19)]['error rate'])[1]/2)},
	    \py{"{:.1e}".format(ttest_rel(data_cs5[(data_cs5['scrambling'] == 0) & (data_cs5['intensity'] == 40)]['error rate'], data_cs5[(data_cs5['scrambling'] == 23)]['error rate'])[1]/2)}, and
	    \py{"{:.1e}".format(ttest_rel(data_cs5[(data_cs5['scrambling'] == 0) & (data_cs5['intensity'] == 40)]['error rate'], data_cs5[(data_cs5['scrambling'] == 27)]['error rate'])[1]/2)}
	    respectively).
	    
	    While this finding indicates limited reliability for our preliminary trials, doubts that our baseline may be unsuited due to differing error rates should also be treated with according scepticism.
	    
	    Reproducibility issues apparent in the data presented in this section - along with broader conclusions to be drawn from these experiments - are further discussed in section \ref{sec:m_pe}.
\chapter{Discussion}
    \section{Preliminary Experiments}\label{sec:d_pe}
	Most conclusions drawn from our preliminary experiments (for instance in section~\ref{sec:m_pe_cs} or section~\ref{sec:r_pe_cs}) are only marginally reliable due to the high inter-run variability noticed throughout section~\ref{sec:r_pe}.
	We believe the chief reason for this is our reduced participant sample (n = 6 to 7), and we would recommend a more thorough examination on a larger population to better support any claims pertaining to accurate reaction time and error rate comparisons.
	
	Some hallmarks, however, have been constant over all our runs, and we believe these deserve to be reported with more pronounced certainty.
	\subsection{Reaction Times over Scrambling Cluster Sizes}
	    Our preliminary experiments show that even when confronted with very high variability in results there is a robust decay of reaction times as scrambling cluster sizes increase. 
\chapter{Meta}
    \begin{figure}[H]
	\[ \sigma_{x,y} = \sqrt{\sigma_{x}^{2}+\sigma_{y}^{2}} = \sqrt{2\sigma_{x}^{2}} \approx \sqrt{2 \cdot (0.68 K)^{2}} \approx 0.96K\]
	\caption{Here we calculate the standard deviation ($\sigma$) of the distance from the original pixel location to the new pixel location following kernel-based scrambling, as detailed in section~\ref{sec:m_vs_si_kbs}. The standard deviation along one axis is defined as $0.68K$ (\SI{68}{\percent} of the scrambling kernel, $K$) following the 68–95–99.7 rule, though experimentally we have found slightly lower values ($\approx 0.6K$).}\label{eq:lrgn}
    \end{figure}
    \begin{figure}[H]
	\[\Pr(\mu - \sigma \le x \le \mu + \sigma) \approx 0.6827 \]
	\[\Pr(\mu - 2\sigma \le x \le \mu + 2\sigma) \approx 0.9545 \]
	\[\Pr(\mu - 3\sigma \le x \le \mu + 3\sigma) \approx 0.9973 \]
	\caption{The general expression of the three-sigma rule, in mathematical notation, for $x$ being an observation from a normally distributed random variable, $\mu$ being the mean of the distribution, and $\sigma$ being its standard deviation. The three-sigma rule (also known as the “68–95–99.7 rule”) states that nearly all values lie within 3 standard deviations of the mean in a normal distribution.}\label{eq:3s}
    \end{figure}
\chapter{Acknowledgements}
    In addition to the faculty advisers and supervisors mentioned in the preamble of this document, we would like to give thanks to the numerous other people with whom we have interacted in the process of writing this thesis.
    Their goodwill and effort made a significant difference pertaining to the quality of this work.
    This list includes but is not limited to:
    \begin{itemize}
	\item Ben Bolker, of McMaster University
        \item Denis A. Engemann, of the Juelich Research Centre in Cologne
	\item Laurent Gautier, of the Technical University of Denmark
	\item Marek Hlavac, of Harvard University
	\item Carina Sauer, of the Central Institute of Mental Health in Mannheim
	\item Gabriela Stoessel, of the Central Institute of Mental Health in Mannheim
    \end{itemize}
    The list in sorted alphabetical order.
\input{footer.tex}
