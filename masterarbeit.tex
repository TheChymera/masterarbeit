\input{header.tex}
\chapter{Summary}
\CatchFileBetweenDelims{\readme}{README.rst}{.. letag}{.. letag>} %adds file content between the specified tags 
\readme
\chapter{Background}
\chapter{Methods}
    \section{Questionnaires}
    We have decided to complement our studies with a series of previously validated and widely used questionnaires.
    These tests could provide insights into the interplay between psychotypes on one side and behavioural parameters or their neurological underpinnings on the other.
    For test presentation we have chosen to use a web-based version of the free and open source (FOSS) software testMaker\cite{testmaker}.
    
    The choice of tests was based on both general relevance to the host group's research, and specific relevance to emotional perception.
    According to these criteria, we have compiled a selection of 8 questionnaires:
    \begin{itemize}
        \item The Autism Spectrum Quotient questionnaire (AQ) \cite{Baron-Cohen2001}
	\item The World Health Organization Adult ADHD Self-Report Scale (ASRS) \cite{Kessler2005}
	\item The 1996 revised Beck Depression Inventory (BDI2) \cite{Beck1996}
	\item The short version of the Borderline Symptoms List (BSL23) \cite{Bohus2009}
	\item The Systemizing Quotient questionnaire (SQ) \cite{Baron-Cohen2003a}
	\item The Empathizing Quotient questionnaire (EQ) \cite{Baron-Cohen2004}
	\item The Schizotypal Personality Questionnaire (SPQ) \cite{Raine1991}
	\item The Emotion Regulation Questionnaire (ERQ) \cite{Gross2003}
    \end{itemize}
    The questionnaires were administered in German language, and translated and validated versions were provided by the test databases of the Central Institute of Mental Health.   
    \section{Visual Stimuli}
	\subsection{Emotional Faces}\label{sec:m.vs.ef}
	\subsection{Scrambled Images}\label{sec:m.vs.si}
	In order to define a baseline for visual recognition we have decided to use trials in which visual input consists of the same pixels as in the emotional trials. 
	To disentangle visual from semantic (emotion) identification we decided to use “scrambled” images - in which pixels from the emotional faces are permuted so as to obfuscate the emotional expressions.
	
	As recognition difficulty of scrambled images should scale proportionally with that of the easy and difficult emotional recognition trials (detailed under section~\ref{sec:m.vs.ef}), scrambling has to be progressively complex.
	Available data ??? suggests that scrambling of smaller image sub-sections (further referred to as “clusters”) makes the images progressively difficult to identify and match to other copies.
	This rationale determined the nature of our scrambling algorithms (detailed below) and was furthermore experimentally validated in preliminary trials (as described in section~\ref{sec:r.pe}).
	
	This was done via a home-brewed Python script written for the purpose of this thesis and openly published on GitHub.
	The script provides both cluster-based and kernel-based scrambling:
	    \subsubsection{Cluster-Based Scrambling}
	    This scrambling functionality recognises the face region of interest (ROI) by scanning for pixel lines with few unique values, and then divides the face ROI in square clusters of predefined sizes.
	    The clusters then get permuted and rewritten in-place on the image - this is done via the \colorbox{vlg}{\texttt{montage2d}} function of the \colorbox{vlg}{\texttt{scikits\_image}} python package 
	    (the function was written and contributed to the package for the benefit of the scientific community as part of this thesis).
	    The image background is then filled with homogeneous values.
	    \subsubsection{Kernel-Based Scrambling}\label{sec:m.vs.si.kbs}
	    This is done by remapping single pixels via the \colorbox{vlg}{\texttt{geometric\_transform}} function of the \colorbox{vlg}{\texttt{scipy}} Python suite. 
	    New positions are computed via a function which adds a random integer in the $[-K;K]$ ($K$ being the scrambling kernel integer) interval to both the X and Y coordinates of the said pixel.
	    Effectively, this redistributes each pixel in an area of $[-K;K]$ around its original position with a standard deviation ($\sigma$) of $\approx 0.96K$ (as calculated in figure~\ref{eq:lrgn}). 
	\subsection{Preliminary Experiments} 
	Preliminary experiments have been conducted in order to establish a proper paradigm for the \textit{main} experiments of the project. 
	Their primary goal is determining which scrambling cluster sizes have reaction times comparable to the \SI{100}{\percent} and \SI{40}{\percent} emotional images (decided upon based on rationales delineated under section~\ref{sec:m.vs.ef}).
	
	For stimulus presentation and data acquisition in these experiments we used a home-brewed Python script written for the purpose of this thesis and openly published on GitHub.
	Our Python script uses the PsychoPy suite\cite{Peirce2008} for specialized, high-precision stimulus rendering and timing.   
	    \subsubsection{Simple Cluster-Based Scrambling}
	    In a first set of trials we have 
\chapter{Results}
	\section{Preliminary Experiments}\label{sec:r.pe}
	In order to establish an experimental paradigm which affords the comparison between emotion recognition and simple visual matching, we need to select stimuli whose matching is correspondingly difficult.
	For the emotion recognition trials, we decided for faces with emotional "concentrations" ??? of 40\% and 100\% (as discussed in section~\ref{sec:m.vs.ef}).
\chapter{Discussion}
\chapter{Meta}
\begin{figure}
\[ \sigma_{x,y} = \sqrt{\sigma_{x}^{2}+\sigma_{y}^{2}} = \sqrt{2\sigma_{x}^{2}} \approx \sqrt{2 \cdot (0.68 K)^{2}} \approx 0.96K\]
\caption{Here we calculate the standard deviation ($\sigma$) of the distance from the original pixel location to the new pixel location following kernel-based scrambling, as detailed in section~\ref{sec:m.vs.si.kbs}. The standard deviation along one axis is defined as $0.68K$ (\SI{68}{\percent} of the scrambling kernel, $K$) following the 68–95–99.7 rule, though experimentally we have found slightly lower values ($\approx 0.6K$).}\label{eq:lrgn}
\end{figure}
\input{footer.tex}
