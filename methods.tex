\chapter{Methods}\label{sec:m}
    \section{Participants}\label{sec:m_p}
	Participants were recruited via flyers and advertisements pinned to information boards in multiple locations at the University of Mannheim and the University of Heidelberg.
	As part of this study, a total number of 12 participants have been tested; 4 male and 8 female.
	Participants were aged 18-29, and had no a priori documented psychiatric or neurologic conditions.
	
	Participants were screened for fMRI testing and gave written informed consent in accordance with the ethical guidelines at the Central Institute of Mental Health.
	\subsection{Trial Procedure}
	    Participants were invited for an appointment after having completed a questionnaire (see section~\ref{sec:m_q}).
	    The appointment commenced with safety instructions and discussion of the information/consent forms.
	    Participants were placed in the fMRI/eye tracking set-up where they completed the behavioural test documented in section~\ref{sec:m_bt} (after approximately \SI{5}{\minute} of anatomical measurements and \SI{17}{\minute} of an other behavioural test performed as part of a different study).
	    Genetic material samples (detailed in section~\ref{sec:m_gms}) were taken after approximately \SI{45}{\minute} of time spent in the scanner.
    \section{Visual Matching Task}\label{sec:m_bt}
	In our current study we are exploring neuronal function in visual emotional perception.
	Our behavioural task of choice to tap into these systems relies on visual recognition and subsequent matching of emotions and/or patterns.
	We call our paradigm a Hariri-style behavioural test, in acknowledgement of the seminal implementations of a similar paradigm by Hariri and colleagues \citep{Hariri2000,Hariri2003}.
	    
	The paradigm consists of a triangular arrangement of three faces - with the bottom two aligned horizontally and the upper face displaced vertically and equidistant from both bottom faces.
	The behavioural test prompts the participant to observe a certain feature (in our case emotion, or - as described in section~\ref{sec:m_vs_si} - pixel arrangement) of the top face, and to select of the bottom faces the one repeating that feature.
	
	In our case bottom face selection was done via the arrow keys of a keyboard (in the preliminary experiments reported under section~\ref{sec:pe_r}) and via a response box (in the fMRI trials reported under section~\ref{sec:m_fmri}).
	The participants received no sort of feedback after reacting to the individual items.
	
	The images were continuously displayed independently of input for \SI{5}{\second} or \SI{4.5}{\second} (in the preliminary experiments), and for \SI{4}{\second} (in the fMRI trials).
	Inter-trial intervals - fixation periods - showed a dot (in the preliminary experiments) or a cross (in the fMRI experiments) and lasted for \SI{2}{\second} or around \SI{4}{\second} respectively.
	
	The higher inter-trial interval duration for the fMRI experiments was chosen to accommodate for a jitter of one fMRI scan repetition time (TR, \SI{2}{\second}).
	Inter-trial intervals have been jittered around the value of \SI{4}{\second} with a maximum jitter amplitude of \SI{2}{\second}.
	\subsection{Trial Sequences}
	    Trial sequences were computed to allow for event-related data analysis.
	    For the fMRI trials we have employed fixed trial sequences to increase signal detection power.
	    These trial sequences were calculated using the Optseq2 \citep{optseq} software package, from a list with permitted and counterbalanced stimulus compositions computed by our Pystim \citep{pystim} script.
	    
	    According to the stimulus compositions and trial sequences each condition (easy emotion matching, difficult emotion matching, easy pattern matching, difficult pattern matching) is represented by 32 trials - in which all pictures depicting a strong emotion are used as a template: 8 $\times$ \{male; female\} $\times$ \{happy; fearful\}.
	    Thereby within each condition every picture is repeated thrice - once as the template, once as the target, and once as the distractor.
	    An exception from this is that in difficult emotion matching trials - where we use images with a lower emotional concentration (see section~\ref{sec:m_vs_ef}) - the templates are still images from the higher emotion concentration set.
	\subsection{Visual Composition}	
	    \begin{wrapfigure}{r}{0.45\textwidth}
		\centering
		\vspace{-1.0cm}
		\scalebox{0.55}{\includegraphics{./img/vis_field.jpg}}
		\caption{Perimetric map of the human field of view \citep{Ruch1960}.
		For measurement the head and eyes were fixed, with the fovea pointing at \SI{0}{\degree} on the cross-hairs.
		The white area affords binocular vision, the black area is completely outside the field of view.}
		\label{fig:m_b_1}
		\vspace{-1.0cm}
	    \end{wrapfigure}
	    
	    In contrast to other formulations\citep{Hariri2000,Hariri2003} of the same paradigm (which keep all 3 stimuli equidistant),
	    we have decided to adapt the positions of stimulus images to what we believe is a better fit for the human field of view.
	    Simply aligning the midpoints of stimulus images to the vertices of an equilateral triangle presents some issues:
	    \begin{itemize}
		\item Not accounting for the aspect ratio of the stimulus images lessens control over the shape and size of the resulting populated visual window (pVW).
		\item Horizontal and diagonal distances are not registered identically by human vision \citep{TerryBahill1975}.
	    \end{itemize}
	    
	    We tackled the above by adapting the pVW to the normal human field of vision aspect ratio.
	    To do so, we took the outermost non-neutral pixels (rather than the centers of the stimulus images) and used these as pVW delimiters in deciding where to position our images.
	    
	    For the following distance specifications we have defined the unit “u” as the height of our stimulus images.
	    Seeing as our stimulus images have a fixed aspect ratio their width is thus consistently \SI[parse-numbers = false]{\frac{3}{4}}{u}.
	    
	    The static human field of view has a complex shape modulated amongst others by facial features (as seen in figure~\ref{fig:m_b_1}).
	    If the outermost light paths fitting figure~\ref{fig:m_b_1} were to be circumscribed by a planar mask in the shape of the rectangle, its height would span approximately \SI{130}{\degree} and its width \SI{180}{\degree}.
	    This gives the rectangular fit for the field of view a width-to-height ratio of 0.72(2).
	    Given the data - and for numerical convenience - we have chosen to position the midpoints of our stimulus images as follows:
	    \begin{itemize}
		\item The distance between the two lower image midpoints is \SI{2}{u}.
		\item The vertical offset of the top image midpoint from the horizontal line connecting the bottom image midpoints is \SI{1}{u}
	    \end{itemize}
	    This gives our pVW a total height of \SI{2}{u} and a total width of \SI[parse-numbers = false]{\frac{11}{4}}{u}.
	    The resulting width-to-height ratio of 0.(72) was deemed a sufficiently close match for the afore mentioned human field of view (as approximated by a rectangle).
	    We thus have reason to believe that our stimulus item distribution is superior to the one implemented by Hariri and colleagues, and we encourage further users of Hariri-style paradigms to consider our observations.  
	    
	    In addition to field of view shape, saccades are an other parameter which offers valuable information on the accuracy and speed with which the eyes track distance (e.g. across a monitor).
	    We are aware that our simplified approximation still does not quantitatively account for vertical saccades being slower \citep{TerryBahill1975} and less accurate \citep{Collewijn1988} than horizontal saccades, 
	    nor for the fact that upward saccades tend to undershoot while downward saccades tend to overshoot \citep{Collewijn1988};
	    however, integrating accurate metrics for these kinetics and determining their relevance for Hariri-style paradigms differs in focus from and far exceeds the scope of our current study.
	    
	    Our resulting pVW was scaled to fit the available display field of the monitor and tilted mirror set-up in our MRI scanner (more on this in section~\ref{sec:m_om_et}).
	    The dimensions (specified in degrees of visual angle) are proportional to the ones mentioned above, scaled for u = \SI{4.24}{\degree}.
	    The midpoint of the pVW (at \SI{0.5}{u} downward from the midpoint of the top image) was aligned to the midpoint of the monitor.
	    A picture of this visual set-up can be seen in figure~\ref{fig:m_b_2}.
	    
	    \begin{figure}[!h]
		\centering{\includegraphics[width=0.9\textwidth]{./img/01F_FE_C100_em100_composite.jpg}}
		\caption{Sample image from the emotional face matching trials in our Hariri-style behavioural task.}
		\label{fig:m_b_2}
	    \end{figure}
			
    \section{Stimulus Images}
	\subsection{Emotional Faces}\label{sec:m_vs_ef}
	    The emotional face stimuli required by our visual matching task (see section~\ref{sec:m_bt}) have been sourced from the NimStim set of facial expressions \citep{Tottenham2009}.
	    We have decided to use different emotion intensities in order to better describe step-wise effects of emotion perception and/or emotion recognition for either pupil dilation time courses, brain activation, reaction times - or interactions thereof.
	    
	    Our emotion intensities of choice are \SI{40}{\percent} and \SI{100}{\percent} - these being referred to as weak (“hard” in the sense of recognition) and strong (“easy” in the sense of recognition) emotion images respectively.
	    The semi-quantitative measure of emotion (in percent) stems from the image morphing used to obtain these images - \SI{40}{\percent} happy images are thus a 4:6 linear combination of \SI{0}{\percent} happy (i.e. neutral) and \SI{100}{\percent} happy eigenfaces \citep{Zhang2008} respectively.
	    
	    The choice of \SI{40}{\percent} was made to obtain the highest possible activation variance both in brain areas presenting discrete (amygdala) or continuous (superior temporal sulcus) sensitivity to increasing emotional concentrations \citep{Harris2012}.
	    Our images comprise 8 male and 8 female faces, displaying happiness and fear, in the two concentrations mentioned above. 
	\subsection{Scrambled Images}\label{sec:m_vs_si}
	    In order to define a pattern recognition (baseline) condition we have decided to use trials in which visual input consists of the same pixels as in the emotional trials.
	    This is especially relevant to pupillometry, for which we would like to ensure condition-independent luminosity.
	    
	    To disentangle pattern identification from semantic (emotion) processing we decided to use “scrambled” images - in which pixels from the emotional faces are permuted so as to obfuscate emotional expressions and facial features.
	    The recognition trials would thus entail matching identical permutations of the same picture.
	    For these trials the template and target are the exact same image.
	    The distractor (i.e. the non-target image) is a different permutation of the pixels which constitute the source image (the image which template/target were also computed from). 
	    
	    Corresponding to the emotion recognition trials, we include two sets of scrambled image trials: “easy” and “difficult”.
	    These are to act as baselines for easy and difficult emotion recognition trials respectively.
	    Recognition difficulty of the scrambled image trials should scale proportionally with that of the easy and difficult emotion recognition trials (detailed under section~\ref{sec:m_vs_ef}).
	    To achieve this, scrambling has to be progressively complex.
	    We assume that scrambling of images via progressively smaller sub-sections (further referred to as “clusters”) makes the images progressively more difficult to identify and match to other copies.
	    This rationale determined the nature of our scrambling algorithms (detailed below) and was also experimentally validated in preliminary trials (as described in section~\ref{sec:pe_d}).
	    
	    Based on the preliminary results to be detailed in section~\ref{sec:pe_r} we have decided to use composite (kernel \textit{and} cluster based) scrambling for our experiment.
	    The scrambling kernel was constant at \SI{6}{\pixel} (following considerations explained in sections~\ref{sec:pe_m_si_cs}) 
	    and “hard” recognition trials had a scrambling cluster size of \SI{10}{\pixel} while “easy” trials had a scrambling cluster of \SI{22}{\pixel} (following preliminary experiment results described in section~\ref{sec:pe_d_rt}.
	    The quality of this baseline is assessed under section~\ref{sec:pe_d_fm}.
	    
	    Scrambling was done via Pyxrand \citep{pyxrand}, a Python script written for the purpose of this thesis and openly published on GitHub.
	    The script provides both cluster-based and kernel-based scrambling:
	    \subsubsection{Cluster-Based Scrambling}\label{sec:m_vs_si_cs}
		The Cluster-Based Scrambling functionality recognises the face region of interest (ROI) by scanning for pixel lines with few unique values, and then divides the face ROI in square clusters of predefined sizes.
		The clusters then get permuted and rewritten in-place on the image - this is done via the \colorbox{vlg}{\texttt{montage2d}} function (of the \colorbox{vlg}{\texttt{scikits\_image}} python package), which  was adapted for this purpose and contributed to the package for the benefit of the scientific community as part of this thesis.
		Finally, the script fills the image background (outside of the ROI) with homogeneous values.
	    \subsubsection{Kernel-Based Scrambling}\label{sec:m_vs_si_ks}
		The Kernel-Bases scrambling variant is performed by remapping every pixel of the image via the \colorbox{vlg}{\texttt{geometric\_transform}} function provided by the SciPy ecosystem \citep{scipy,Oliphant2007}.
		New positions are computed via a function which adds a random integer in the $[-K;K]$ ($K$ being the scrambling kernel integer) interval to both the X and Y coordinates of the said pixel.
		Effectively, this redistributes each pixel in an area of $[-K;K]$ around its original position with a standard deviation ($\sigma$) of $\approx 0.96K$ (as calculated in figure~\ref{eq:lrgn}).
    
    \section{Occulometry}\label{sec:m_om}
	As part of our experiments we have measured occulometric parameters of our participants during behavioural trials performed inside the MRI scanner.
	Occulometric data was captured via an infra-red SemsoMotoric Instruments (SMI) “IVIEW X\textsuperscript{\small\texttrademark} MRI-LR” \SI{60}{\hertz} fMRI-compatible eye tracker.
	The light source needed to afford signal for the eye-tracking camera is built-in and shines infra-red light.
	This avoids triggering the pupillary light reflex \citep{Ellis1981} and distorting the pupil diameter data.

	\subsection{Eye Tracking}\label{sec:m_om_et}
	    More accurately, we are performing “gaze tracking” rather than “eye tracking”, as the system measures the gaze coordinates rather than eye-in-head angles.
	    We use “eye tracking” as a general term for both measurements, keeping with the trend in both industry specifications \citep{Bojko2006} as well as peer-reviewed publications \citep{Kirk2013}.
	    
	    \subsubsection{Data Acquisition}
		Gaze coordinates are calculated by the firmware provided with our eye tracker, based on the position of the corneal light source reflection relative to the pupil.
		As our stimulus presentation does not yet incorporate a drift correction mechanism, our eye-tracking data is subject to gross movement artefacts.

	\subsection{Pupillometry}\label{sec:m_om_pm}
	    \subsubsection{Data Acquisition}
		From the data provided by the aforementioned hardware we are separately extracting the pupil diameter.
		The pupil is identified by the firmware of our eye tracker via the contrast between pupil (dark) and iris (bright under infra-red light, regardless of eye color)
		Pupil area is approximated by the firmware via an edge detection algorithm with a perimeter-to-area ratio constraint
		(keeping the detected area as close as possible to a circular form).

	    \subsubsection{Data Analysis}
		Pupillometric data is analysed and plotted via specialized the faceOM script suite \citep{faceOM}.
		
		Our data is slightly distorted due to a number of factors: most prominently subjects' eyelids covering parts of the pupils; or dark make-up of some female participants being registered as pupil area.
		These distortions are not critical for gaze point calculation (as long as they remain constant over the course of calibration and measurement); 
		but they do interfere non-linearly with the firmware-calculated pupil area 
		(pupil area lost to eyelid coverage, for instance, scales quite complexly with overlap distance - see figure~\ref{eq:cs}).
		To avoid such complex data-reconstruction algorithms we are considering using the pupil horizontal diameter as a reference measure for the pupil dilation response in lieu of the actual area.
		These considerations are further addressed based on experimental results under section~\ref{sec:r_p}.
	    
    \section{Functional MRI}\label{sec:m_fmri}
	In the present study we are using BOLD response data obtained via fMRI as a proxy for neuronal activation.
	This data was acquired over a \SI{3}{\tesla} Siemens “TIM Trio” MRI scanner at the Central Institute of Mental Health in Mannheim.
	\subsection{Data Acquisition}
	    176 anatomical slices were acquired in the sagital plane via a T1-MPRAGE sequence \citep{Brant-Zawadzki1992}, with a repetition time (TR) of \SI{1.57}{\second}, an echo time of \SI{2.7}{\milli\second}, and a flip angle of \SI{15}{\degree}.
	    The slice thickness and the X and Y-axis resolution (relative to the acquisition plane) of the tomographic data were \SI{1}{\milli\metre} (\SI{1}{\milli\metre$^3$} voxel size).
	    
	    Functional data was acquired via T2* weighted echo-planar imaging (T2* EPI), with a repetition time (TR) of \SI{2}{\second}, an echo time of \SI{30}{\milli\second}, and a flip angle of \SI{80}{\degree}.
	    A total of 33 transversal slices were acquired in descending (dorsocaudal) order, at a slice thickness of \SI{3}{\milli\metre}, and an X and Y-axis resolution of again \SI{3}{\milli\metre} (\SI{27}{\milli\metre$^3$} voxel size).
	\subsection{Data Processing}\label{sec:m_fmri_dp}
	    The resulting fMRI data in \colorbox{vlg}{\texttt{.dcm}} (DICOM) format was converted to the more convenient \colorbox{vlg}{\texttt{.nii}} (NIfTI) via the MRIconvert \citep{MRIconvert} software package.
	    Unless otherwise specified, all subsequent NIfTI file processing and analysis was performed via SPM8 \citep{spm} - a MATLAB\textsuperscript{\small\textregistered}-based brain imaging analysis suite.
	    
	    Further preprocessing steps have been done in accordance with the established guidelines of the group, and are documented (in simplified form) as part of the faceOM script suite \citep{faceOM} - which was written for the purpose of this thesis and openly published for the benefit of researchers pursuing similar analysis.
	    
	    \begin{wrapfigure}{r}{0.55\textwidth}
		\centering
		\vspace{-0.7cm}
		\scalebox{0.31}{\includegraphics{./img/LC_pic.png}}
		\caption{Mask for the LC based on MNI coordinates extracted from in-vivo MRI localization \citep{Keren2009}. The mask was smoothed and the dynamic range remapped to [-1;1].}
		\label{fig:m_fmri_dp}
		\vspace{-1.1cm}
	    \end{wrapfigure}

	    Slice timing correction was done according to the EPI sequence - in descending order of slices, non-interlaced, and with the \nth{17} slice (the middle slice) as the reference.
	    Normalization of the data was achieved by coregistration of the EPI to the anatomical scan, segmentation of the anatomical scan, coregistration to the standard MNI (Montreal Neurological Institute) template, and subsequent mapping of the EPI to these parameters. 
	    Data was realigned to the mean image, and smoothing was done with a 6 $\times$ 6 $\times$ \SI{6}{\milli\metre} full width half maximum kernel.
	    
	    Data was analysed by a first-level computation of per-participant contrasts, and a subsequent second-level statistical analysis of these contrasts, resulting in an activation probability map.
	    First level analyses used movement regressors as regressors of no interest, and per-condition lists of trial onsets as regressors of interest.
	    The conditions which we have compared are:
	    \begin{itemize}
		\item Hard vs. easy trials (integrating both emotion and pattern matching)
		\item Easy emotion matching trials vs. easy pattern matching trials
		\item Hard emotion matching trials vs. hard pattern matching trials
		\item Hard emotion matching trials vs. easy emotion matching trials.
	    \end{itemize} 

	    Additionally, pupil diameter time course data obtained from infra-red pupillometry (see section~\ref{sec:m_om_pm}) was down-sampled to a rate of \SI{0.5}{\hertz} (to match the functional TR) and used as a signal regressor.
	    As it is yet unclear what temporal relationship the pupil dilation response and LC activation have in humans, we implement this kind of regressor in a series of different variations:
	    \begin{itemize}
		\item Raw pupil dilation time course
		\item Pupil dilation time course convolved with the hemodynamic response function (HRF)
		\item Numerical first differential of the pupil dilation time course
		\item Numerical first differential convolved with the hemodynamic response function (HRF)
	    \end{itemize}
	    
	    The numerical first derivative (first differential) of the raw pupil area time course was computed by the faceOM script suite via the \colorbox{vlg}{\texttt{DataFrame.diff()}} function of the Pandas \citep{pandas} library.
	    The regressors were convoluted with the haemodynamic response function (HRF) via SPM8.
	    
	    The second-level (significance determining) analysis step was done both with and without a locus coeruleus mask.
	    We used a mask drawn for the purpose of this thesis in the MARINA program \citep{Walter2003} and based on in-vivo magnetic resonance localization of the LC available in peer-reviewed literature \citep{Keren2009}.
	    Three representative cross sections of our mask are presented in figure~\ref{fig:m_fmri_dp}
		
	    Coordinates of non-ROI (region of interest) brain activation clusters were assigned anatomical descriptions based on the Wake Forest University (WFU) PickAtlas \citep{Maldjian2003}; 
	    and the anatomical descriptions are given as International Consortium for Brain Mapping (ICBM) labels.
	    In cases where the coordinates exceed the bounds of the ICBM we convert them to Talairach coordinates and fall back to the Talairach atlas labelling.
	    Unless otherwise specified all coordinates are given in Montreal Neurological Institute (MNI) standard space.
	    
    \section{Statistical Analysis}\label{sec:m_sa}
	For the analysis of our data we have implemented a number of statistic methods from different software packages.
	Our choice of methods took into account transparency and reproducibility, and we have thus - whenever possible - avoided closed-source and/or graphical user interface based software (such as for example MATLAB\textsuperscript{\small\textregistered} or SPSS\textsuperscript{\small\textregistered}), and have published our data analysis scripts in full on-line.
	Many of the statistical analysis functions are called at compile time from the \LaTeX\ code of this document, meaning that many of our statistical results are live.
	
	For ease of overview we have compiled a short index with explicit naming and descriptions of our statistical analysis tools.
	The methods detailed hereafter are referred to in further sections of this document simply by their subsection names.
	\subsection{Paired T-Test}\label{sec:m_sa_pt}
	    The t-test is a method used to test the null hypothesis that the population means of two compared sample groups are equal.
	    The most frequently reported resulting measure of the test (the \textit{p}-value) represents the probability of the presented data being observed if the null hypothesis is true. 
	    The t-test makes a small series of assumptions, among which is the independence of data points and the normal distribution of data.
	    
	    Our experiments mainly rely on testing the same population on various conditions.
	    To adapt the t-test to this constraint (where data points are no longer independent) we use the paired sample t-test (also known as a related sample t-tests).
	    We implement this statistical method via the \colorbox{vlg}{\texttt{scipy.stats.mstats.ttest\_rel}} function of the SciPy ecosystem \citep{scipy,Oliphant2007}.
	    
	    Unless otherwise specified, in order to avoid inflating our significance we are testing per-participant means instead of raw data points.
	    This also affords the advantage of averting undocumented violations of the normal distribution assumption of the t-test -
	    since means of even non-normally distributed populations tend to be normally distributed.
	    Aditionally, unless otherwise specified we are conducting two-tailed t-tests, as we are trying to minimize a priori assumptions in the explorative context of our study. 
	\subsection{Standard Error of the Mean}\label{sec:m_sa_se}
	    Confusion around appropriate usage of the standard deviation (SD) and standard error of the mean (SEM) is a significant quality deficit and cause of imprecision in modern research \citep{Nagele2003}.
	    SD is a measure of single data point variability and tends to be constant over increasing sample size, 
	    whereas SEM is a measure of the data sample mean reliability and tends to decrease as the sample size increases \citep{Altman2005,Streiner1996}.
	    
	    As we are in more cases concerned with per-category means than data point variability, our error bars of choice represent the SEM by default.
	    Cases in which graphical depiction of the SD is helpful in understanding the data at hand are described accordingly and explicitly.
	    
	    In our purpose-built scripts, faceRT \citep{faceRT} and faceOM \citep{faceOM}, we use the \colorbox{vlg}{\texttt{scipy.stats.sem}} and \colorbox{vlg}{\texttt{numpy.std}} functions of the SciPy ecosystem \citep{scipy,Oliphant2007} -
	    for the standard error of the mean and standard deviation respectively.
	\subsection{ANOVA}\label{sec:m_sa_a}
	    ANOVA, the so-called analysis of variance, is a very popular set of statistical methods in psychology and related fields.
	    Formally, ANOVA tests the null hypothesis that the population means of multiple groups are equal.
	    
	    The method, however, has a series of limitations:
	    \begin{itemize}
		\item ANOVA provides no information as to the group whose mean is outside the range of other groups, nor does it indicate whether this is one group or a number of groups.
		\item ANOVA does not deal well with missing measurements (which motivates scientists to perform ANOVA on means rather than on raw data points - and thus discard valuable information) \citep{Gueorguieva2004}.
		\item The assumptions made by ANOVA - independence of observations, normal distribution of residuals, and homoscedasticity \citep{Anderson1996} - are likely to be violated due to autocorrelation whenever using raw data points. 
	    \end{itemize}
	    Overall, statisticians involved in biology and psychology recommend against ANOVA and point to mixed models (see section~\ref{sec:m_sa_lm}) as a substitute \citep{Baayen2008,Gueorguieva2004}. 
	    In light of these recommendations we are only using ANOVA in cases where raw data point sets represent per-participant or binned values 
	    (such as in questionnaires, or in error rate analyses - as seen under section~\ref{sec:pe_r_ss}).
	    
	    In those cases where we do choose to resort to ANOVA, we have to consider that the standard one-way ANOVA assumes all data points are independent.
	    For our applications - where we are testing the same sample of participants over a number of conditions - the appropriate variation of ANOVA would be the repeated measure ANOVA \citep{Gueorguieva2004}.
	    Our ANOVA function of choice is \colorbox{vlg}{\texttt{stats::aov}} \citep{Chambers1992} from the R statistical environment \citep{R}.
	\subsection{Mixed Models}\label{sec:m_sa_lm}
	    Current literature \citep{Baayen2008} recommends mixed models over ANOVA's standard linear model in order to describe the effect of multiple experimental conditions on measurements.
	    
	    We fit mixed models to our data via the \colorbox{vlg}{\texttt{lme4::lmer}} \citep{Bates2005,Bates2007} R function.
	    This function - though one of the most powerful current tools for mixed modelling - fails to provide \textit{p}-values.
	    The author has justified this \citep{Bates2006}, and generally recommended against using \textit{p}-values with such models.
	    In spite of this recommendation, we hold that \textit{p}-values are a stringent standard in quantifying the reliability of scientific observations -
	    and are using the alternate \colorbox{vlg}{\texttt{nlme::lme}} \citep{Pinheiro2013} function whenever we believe \textit{p}-values are needed.
	    
	    We are using these functions in many instances to compute mixed models \textit{live} in this document.
	    For this purpose we are using the interstats \citep{interstats} software package as a wrapper.
	    This software was written for the purpose of this thesis and openly published on GitHub \citep{github}.
	\subsection{Pearson's \textit{r}}
	    We use “Pearson's \textit{r}” for the quantitative description of variable dependence - in cases where we believe this dependence can be reasonably approximated by linear correlation.
	    Pearson's \textit{r} is a century-old method with widespread use in the empirical sciences \citep{Stigler1989}.
	    It is however noteworthy that it is only meaningful for linear correlations (or nonlinear correlations with significant linear components).
	    
	    To test for this statistical measure we are using the \colorbox{vlg}{\texttt{scipy.stats.pearsonr}} function from the Scipy ecosystem \citep{scipy,Oliphant2007}.
	    To determine the linear equation of regression lines we use the least-squares solution of the linear matrix equation of our measurement vectors.
	    This is performed by the \colorbox{vlg}{\texttt{numpy.linalg.lstsq}} function provided by the Scipy suite.
    \section{Questionnaires}\label{sec:m_q}
	We have decided to complement our studies with a series of validated and widely used questionnaires.
	These tests could provide valuable insight into the interplay between psychotypes on one side and behavioural parameters or their neurophysiological underpinnings on the other.
	For test presentation we have decided on a web-based approach, for which we have used a web-enabled version of the free and open source (FOSS) software testMaker \citep{testmaker}.
	
	The choice of tests was based on both general relevance to the host group's research, and specific relevance to emotional perception.
	According to these criteria, we have compiled a selection of 8 questionnaires:
	\begin{itemize}
	    \item The Autism Spectrum Quotient questionnaire (\textbf{AQ}) \citep{Baron-Cohen2001}
	    \item The World Health Organization Adult ADHD Self-Report Scale (\textbf{ASRS}) \citep{Kessler2005}
	    \item The 1996 revised Beck Depression Inventory (\textbf{BDI2}) \citep{Beck1996}
	    \item The short version of the Borderline Symptoms List (\textbf{BSL23}) \citep{Bohus2009}
	    \item The Empathizing Quotient questionnaire (\textbf{EQ}) \citep{Baron-Cohen2004}
	    \item The Systemizing Quotient questionnaire (\textbf{SQ}) \citep{Baron-Cohen2003a}
	    \item The Emotion Regulation Questionnaire (\textbf{ERQ}) \citep{Gross2003}
	    \item The Schizotypal Personality Questionnaire (\textbf{SPQ}) \citep{Raine1991}
	\end{itemize}
	The questionnaires were administered in German language, and translated and validated versions were provided by the test databases of the Central Institute of Mental Health.
	Translation references are as follows:
	\begin{itemize}
	    \item \textbf{AQ} - Translated by C.M. Freitag, Homburg
	    \item \textbf{ASRS} - Translated by Dr. P. Kirsch, JLU Gießen 2005
	    \item \textbf{BDI2} - Translated by Pearson Assessment \& Information GmbH (Frankfurt/M. 2010)
	    \item \textbf{BSL23} - Translated at the Department of Psychosomatic Medicine and Psychotherapy, Central Institute of Mental Health.
	    \item \textbf{EQ} - Translated by Dipl.-Psych. Jörn de Haen
	    \item \textbf{SQ} - Translated by Dipl.-Psych. Jörn de Haen
	    \item \textbf{ERQ} - Translated by Brigit Abler and Henrik Kessler \citep{Abler2009}
	    \item \textbf{SPQ} - Tranlsated by Klein, Andersen, and Jahn \citep{Klein1997}
	\end{itemize}
	The questionnaires were evaluated with questioPy\citep{questiopy}, a Python script written for the purpose of this thesis and openly published on GitHub\citep{github}.
    \section{Genetic Material Samples}\label{sec:m_gms}
	We have harvested mouth epithelial cells from our participants for prospective genotyping.
	The cells were obtained as a saliva sample collected via an Oragene\textsuperscript{\small\textregistered}-DNA “Self-Collection Kit OG-500” (DNA Genotek Inc. Ottawa, Ontario, Canada).
	
	The resulting suspensions are stored at room temperature (\SIrange{15}{30}{\celsius}) and shall be genotyped whenever the participant sample reaches an adequate size for genetic testing (\SI{62}{participants}).
	The choice to resort to saliva rather than whole-blood lymphoblast testing was made in light of cost and safety considerations, though we are aware of the method's limited reliability \citep{Philibert2008}.
