\input{header.tex}
\input{summary.tex}
\input{background.tex}
\input{methods.tex}
\chapter{Results}                                                                          
    \section{Preliminary Experiments}\label{sec:r_pe}
	In order to establish an experimental paradigm which affords the comparison between emotion and pattern recognition, we need to select stimuli whose matching is correspondingly difficult.
	For the emotion recognition trials, we decided on faces with emotional concentrations of 40\% and 100\% (as discussed in section~\ref{sec:m_vs_ef}).
	
	In the following experiments we examine reaction times and error rates for the recognition of progressively scrambled images in comparison to the recognition of our two emotional concentrations.
	\py[pe1]{fig_pe1}
	
	The reaction time figures which we present include both per-participant and population sample bar plots (the latter marked as “ALL”).
	As our sample sizes are reduced, we acknowledge the limited reliability with which we can make claims pertaining to the general population. 
	
	We would, however, also like to indicate that both inter-trial and inter-participant variability is high (as seen in figure~\ref{fig:r_pe1}).
	The accuracy of the determined mean (reflected by the standard errors in figure~\ref{fig:r_pe1}) could be improved by more extensive testing - though the usefulness of doing so with respect to our search for an optimal baseline would be limited.
	Regardless of the quality of our mean, reaction times on individual trials would still often coincide even for less stringent or sub-optimal choices of a baseline.  
	This would happen on the trial level, as well as on the \textit{mean} level for certain participants.
	Please see the standard deviations in figure~\ref{fig:r_pe1} for a graphical representation of where approximately \SI{68}{\percent} of data points reside for each condition.
	\subsection{Simple Cluster-Based Scrambling}\label{sec:r_pe_ss}
	    For the results presented in figure~\ref{fig:r_pe_ss1} we have used a simple cluster-based scrambling algorithm and a hand-written balanced stimulus sequence 
	    (both detailed in section~\ref{sec:m_pe_ss}).
	    \py[pe_ss1]{fig_pe_ss1}
	    
	    Figure~\ref{fig:r_pe_ss1} confirms our assumption that reaction times for the difficult emotion recognition condition are significantly different from (one-tailed related sample t-test \textit{p}-value of 
	    \py[pe_ss1]{tex_nr(ttest_rel(data_pe_ss1[(data_pe_ss1['scrambling'] == 0) & (data_pe_ss1['intensity'] == 40)]['RT'], data_pe_ss1[(data_pe_ss1['scrambling'] == 0) & (data_pe_ss1['intensity'] == 100)]['RT'])[1]/2)}),
	    and higher than for the easy condition.
	    The results also support our assumption of the presence of a downward trend in reaction times over increasing scrambling cluster sizes.
	    Additionally we see the emergence of a plateau-phase over higher scrambling cluster sizes.
	    
	    To test the validity of this tentative interpretation, we have fitted a mixed model to the reaction times of pattern recognition trials.
	    The model presented in table~\ref{tab:r_pe_ss1} supports our interpretation by showing that:
	    \begin{itemize}
		\item There is a negative progression among the first three factors, with all factor values lying outside of the confidence intervals of all others.
		\item For the subsequent 4 factors there is no obvious trend, with all factor values lying within each other's confidence intervals. 
	    \end{itemize}
	    \py[pe_ss1]{lm(data=data_pe_ss1[(data_pe_ss1['scrambling'] != 0)], fixed='RT~COI', random='ID', title='Mixed model fitted to the raw data depicted in figure~\\ref{fig:r_pe_ss1}. Our factors are the conditions of interest (COI), excluding the emotion recognition trials. The mod el chooses the first factor (scrambling-06) as the base intercept. Note the downwards trend for the first 3 factors and that the last 4 factors\' values lie within each other\'s confidence intervals.', label='tab:r_pe_ss1')}

	    This plateau suggests that scrambling cluster sizes of \SI{14}{\pixel}, \SI{18}{\pixel}, \SI{22}{\pixel}, and \SI{26}{\pixel} possessed the same recognition-relevant quality, which the \SI{6}{\pixel} scrambling cluster - at least - did not.
	    If this purported feature is to explain the reaction time distribution, it should emerge at a scrambling cluster size of around \SI{10}{\pixel}.
	    Visual scrutiny of the images suggested that the emergent feature may be recognizable facial elements - more precisely the eyes (with the iris having an outer diameter of approximately \SI{8}{\pixel}).
	    To test our hypothesis and define a more appropriate baseline for simple visual recognition, we devised a new set of experiments - described in section~\ref{sec:m_pe_cs}, and analyzed in section \ref{sec:r_pe_cs}.
	    
	    Though the images used in this first experiment run shall not be used as the definitive baseline, we conduct a baseline search to use for our further analysis in this section.
	    As we can see in table~\ref{tab:r_pe_ss2} pattern recognition of figures produced with a \SI{6}{\pixel} scrambling cluster recommends itself as the least unlikely (to keep with accurate terminology of the hypothesis testing we are performing) condition to share an equivalent baseline with difficult emotion recognition.
	    Similarly, it is pattern recognition of images processed with a \SI{22}{\pixel} scrambling cluster which we choose as our best-guess reaction time equivalent for easy emotion recognition.
	    \py[pe_ss1]{p_table(data_pe_ss1, ['COI', 'emotion-easy', 'emotion-hard'], ['scrambling', 6, 10, 14, 18, 22, 26], refcap='Proposed best estimators (scrambling cluster sizes in \\SI{}{px})', caption='Two-tailed paired sample t-test \\textit{p}-values for the data in figure~\\ref{fig:r_pe_ss1}; testing the probability of our measured observations if the compared conditions share the same mean. Note that - though unorthodox - this test does not lose accuracy due to multiple comparisons. As we are looking for the group least likely to reject the null hypothesis, multiple comparison actually makes the test more stringent.', label='tab:r_pe_ss2')}
	    
	    In addition to the reaction time per-category analysis, we attempted to gauge the maximal window for reaction times which we should provide in further experiments.
	    For this analysis we have selectively looked at reaction times for images processed with \SI{0}{\pixel}, \SI{6}{\pixel}, and \SI{22}{\pixel} scrambling clusters (due to considerations detailed above and documented in table~\ref{tab:r_pe_ss2}).
	    \py{fig_pe_ss2}
	    
	    Figure~\ref{fig:r_pe_ss2} shows the reaction time distribution for the said subset of trials.
	    The mean value ($\mu$) for the reaction times is approximately \pySI{np.around(data_pe_ss2['RT'].mean(), decimals=2)}{\second}, 
	    and the standard deviation ($\sigma$) of the fitted normal distribution is approximately \pySI{np.around(np.std(data_pe_ss2['RT']), decimals=2)}{\second}.
	    Based on the three-sigma rule (see figure~\ref{eq:3s} for a general expression) we have decided that a reaction time cut-off of \pySI{np.around(data_pe_ss2['RT'].mean()+3*np.std(data_pe_ss2['RT']), decimals=2)}{\second} should not statistically distort our data.
	    
	    We have also tried to analyse the error rates in our Hariri-style matching task in order to extract more information on the appropriateness of our proposed baselines.
	    The results herefor are presented in figure~\ref{fig:r_pe_ss3}.
	    \py[pe_ss3]{fig_pe_ss3}

	    Our error rate distribution over categories proves difficult to interpret.
	    It is obvious for one thing that error rates for most categories vary greatly among participants.
	    It also seems possible that emotion recognition in easy trials may be more robust than image matching in all other categories (owing to the constant null rate).
	    \py[pe_ss3]{av(data_pe_ss3, 'ER~COI+Error(ID)', title='One-way repeated measure ANOVA table for the data depicted in figure~\\ref{fig:r_pe_ss3}. The $Pr(>F)$ value indicates the adjusted probability of observed results occurring by chance if the means of our defined categories are equal.', label='tab:r_pe_ss3')}
	    
	    To test this assumption we have performed an ANOVA (see table~\ref{tab:r_pe_ss3}).
	    The results of this analysis prove equally difficult to interpret. While the $Pr(>F)$ (nominally, \textit{p}) value lies above the popular significance threshold of \textit{p}<0.05, 
	    it is still below the more lax (though not unheard of) \textit{p}<0.1 threshold.
	    
	    To further document this analysis we have fitted a mixed model with computed \textit{p}-values (see table~\ref{tab:r_pe_ss3a}).
	    This model indicates that, while it could find 3 factor values which were significantly different than the base intercept, it found 4 which were not.
	    Note that it is not abnormal for single comparison to find significant effects though ANOVA does not (more on this in section~\ref{sec:m_sa}).
	    We take this as supporting our decision no not reject the null hypothesis of all categories sharing the same mean.
	    \py[pe_ss3]{lm(data_pe_ss3, fixed='ER~COI', random='ID', title='Mixed model fitted to the raw data presented in figure~\\ref{fig:r_pe_ss3}. The model chooses easy emotion recognition (incidentally our category suspected of deviating from the mean) as the base intercept. The confidence interval for the factor values is annotated in the parentheses.', label='tab:r_pe_ss3a', model='nlme')}
	\subsection{Composite (Kernel-and-Cluster-Based) Scrambling}\label{sec:r_pe_cs}
	    Based on the results detailed in section ~\ref{sec:r_pe_ss} we have performed additional experiments using a series of improved method choices (detailed in section \ref{sec:m_pe_cs}).
	    
	    We have re-run experiments with kernel-and-cluster scrambled images a number of times.
	    This was based on progressively noticing parameters in need of adjustment so as to create a most faithful emulation of the in-scanner visual experience.
	    For the sake of brevity, we will only discuss the results of our final and most faithful emulation here in this section.
	    Data from the previous runs is appended for the sake of completion under section~\ref{sec:sm_ndper}.
	     
	    \py[pe_cs3]{fig_pe_cs3} 
	    
	    Figure~\ref{fig:r_pe_cs3} presents the the data from the final preliminary experiment run - which best approximates the visual input participants will be receiving in the fMRI trials, and which we trust above all others as an appropriate guideline for further experimental choices.
	    An obvious feature here is the disappearance of the plateau phase, which supports our hypothesis regarding its emergence due to facial features with a horizontal and/or vertical spatial frequency of around \SI{8}{\pixel} (formulated in section~\ref{sec:r_pe_ss}; this is only partly corroborated by other test runs, as seen in supplementary figures~\ref{fig:r_pe_cs1} and~\ref{fig:r_pe_cs2}).
	    
	    We have tried to determine the best-guess equivalent for our categories of interest - in terms of reaction times - using a t-test \textit{p}-value listing (see table~\ref{tab:r_pe_cs3}).
	    Our results indicate that pattern recognition of figures produced with a \SI{11}{\pixel} scrambling cluster is the least unlikely condition to share an equivalent baseline with difficult emotion recognition.
	    Similarly, it is pattern recognition of images processed with a \SI{23}{\pixel} scrambling cluster which we choose as our best-guess reaction time equivalent for easy emotion recognition.
	    \py[pe_cs3]{p_table(data_pe_cs3, ['COI', 'emotion-easy', 'emotion-hard'], ['scrambling', 11, 15, 19, 23, 27], refcap='Proposed best estimators (scrambling cluster sizes in \\SI{}{px})', caption='Table of two-tailed paired sample t-test \\textit{p}-values for the data in figure ~\\ref{fig:r_pe_cs3}, testing the probability of our observations if the compared conditions share the same mean. Note that - though unorthodox - this test does not lose accuracy due to multiple comparisons. As we are looking for the group least likely to reject the null hypothesis, multiple comparison actually makes the test more stringent.', label='tab:r_pe_cs3')}
	    
	    It should be noted that though not significantly different, both of these simple visual recognition reaction times undershoot their counterpart emotion recognition reaction times.
	    In light of this we have decided to use a \SI{6}{\pixel} kernel- and \SI{10}{\pixel} cluster-scrambled image baseline for difficult emotion recognition, 
	    and a \SI{6}{\pixel} kernel- and \SI{22}{\pixel} cluster-scrambled image baseline for easy emotion recognition.
	    
	    \py[pe_cs5]{fig_pe_cs5}
	    
	    To re-examine potential weaknesses of our baseline concept (related to error rate distribution and further discussed in section~\ref{sec:r_pe_ss}), we have also analysed error rates for this last and methodologically most faithful preliminary experiment run.
	    Figure~\ref{fig:r_pe_cs5} presents a slightly divergent picture from what we have seen in figure~\ref{fig:r_pe_ss3}.
	    For one thing, the easy emotion recognition condition no longer has a null error rate.
	    This time, however, there is a significant error rate divergence for one of our conditions compared to all of the others (as seen in figure~\ref{fig:r_pe_cs5}, and verified in table~\ref{tab:r_pe_cs5}).
	    \py[pe_cs5]{av(data_pe_cs5, 'ER~COI+Error(ID)', title='One-way repeated measure ANOVA table for the data depicted in figure~\\ref{fig:r_pe_cs5}. The $Pr(>F)$ value indicates the adjusted probability of observed results occurring by chance if the means of our defined categories are equal.', label='tab:r_pe_cs5')}
	
	    While this finding confirms limited reliability for all of our our preliminary trials, resulting doubts that our baseline may be unsuited due to differing error rates should also be treated with according scepticism.
	    
	    We also analyse whether our slightly over-cropped reaction time window seems to distort our data.
	    In section~\ref{sec:m_pe_cs} we have detailed our decision to settle for a \SI{4}{\second} reaction time window, the predicted ideal cropping being \pySI{np.around(data_pe_ss2['RT'].mean()+3*np.std(data_pe_ss2['RT']), decimals=2)}{\second}, as calculated in section~\ref{sec:r_pe_ss}.
	    In figure~\ref{fig:r_pe_cs4} we see that reaction times seem to fade approximately \SI{0.5}{\second} before our cut-off, and the fitted normal distribution does not indicate expected results beyond that point.
	    We take these results as confirmation that our selected cut-off time is not over-stringent for the selected conditions (easy and hard emotion recognition, plus visual recognition of scrambled images processed with a \SI{6}{\pixel} scrambling kernel and \SI{11}{\pixel} and \SI{23}{\pixel} scrambling cluster respectively).	    
	    \py[pe_cs4]{fig_pe_cs4}
	    
	    Reproducibility issues apparent in the data presented in this section - along with broader conclusions to be drawn from these experiments - are further discussed in section \ref{sec:d_pe}.
	    
	\subsection{Final Model Quality Assessment}
	    We have decided to fit a mixed model to both the reaction times and error rates of our last-run data in order to check the quality of the baseline we have proposed as the best-guess solution based on the data detailed above.
	    For this model we are defining “scrambling” and “difficulty” as our factors of interest (independent variables).
	    Easy emotion recognition and its corresponding pattern recognition variant (\SI{6}{\pixel} kernel- and \SI{23}{\pixel} cluster-scrambled) are defined as easy,
	    while hard emotion recognition and its corresponding pattern recognition variant (\SI{6}{\pixel} kernel- and \SI{11}{\pixel} cluster-scrambled) are defined as hard.
	    
	    For our ideal baseline we would be expecting a null value for the scrambling factor and a positive value for the difficulty factor in the reaction time model.
	    In the error rate model we would be expecting a null value for the scrambling factor and either a null or positive value for the difficulty factor.
	    \py[pe_cs5]{lm(data=data_pe_cs5[(data_pe_cs5['COI'] == 'scrambling-11') | (data_pe_cs5['COI'] == 'scrambling-23') | (data_pe_cs5['COI'] == 'emotion-hard') | (data_pe_cs5['COI'] == 'emotion-easy')], fixed='ER~difficulty*scrambled', random='ID', title='Mixed model fitted to the raw error rate data depicted in figure~\\ref{fig:r_pe_cs5}. Our factors are scrambling (yes or no) and difficulty (easy or hard). The model chooses "easy" and "not scrambled" as the intercept value. Difficulty and scrambling show no significant factor value, while their interaction does. The base intercept is not significant meaning that the base error rate of the model may not be be reliably considered to lie above zero.', label='tab:r_pe_cs5s')}
	    \py[pe_cs3]{lm(data=data_pe_cs3[(data_pe_cs3['COI'] == 'scrambling-11') | (data_pe_cs3['COI'] == 'scrambling-23') | (data_pe_cs3['COI'] == 'emotion-hard') | (data_pe_cs3['COI'] == 'emotion-easy')], fixed='RT~difficulty*scrambled', random='ID', title='Mixed model fitted to the raw reaction time data depicted in figure~\\ref{fig:r_pe_cs3}. Our factors are scrambling (yes or no) and difficulty (easy or hard). The model chooses "easy" and "not scrambled" as the intercept value. The base intercept is significant, indicating reaction times of the model are above zero. The difficulty factor is also positive and significant meaning that difficulty adds to response latency. All other factors are not significantly other than zero.', label='tab:r_pe_cs3s')}
	    
	    Tables~\ref{tab:r_pe_cs5s} and~\ref{tab:r_pe_cs3s}, provide a succinct and reliable validation for our proposed baseline model.
	    
	    Table~\ref{tab:r_pe_cs5s} recommends the choice of scrambling as a suitable baseline creation method for our task.
	    The factor determined for this dimension is null, meaning that scrambling does not compromise the behavioural equivalence of emotional testing and baseline trials.
	    Difficulty does also not seem to impact the error rate, which we consider fortunate, though a higher error rate for difficult trials would also have been acceptable.
	    The interaction effect of difficulty and scrambling is probably an effect of the deviating hard emotion recognition category we have observed under figure~\ref{fig:fig_pe_cs5}, with an inverted balance due to our model's choice of a base intercept.
	    
	    We would like to point out that the null residual variance of our model indicates that it could be overparameterized.
	    We have tried a series of alternate models (“scrambled” and “difficulty” - not including interaction -, “scrambled” alone and “difficulty” alone), but all resulted in null residual variance - 
	    see tables~\ref{tab:r_pe_cs5s1}, \ref{tab:r_pe_cs5s2}, and~\ref{tab:r_pe_cs5s3}.
	    
	    The reaction time model (table~\ref{tab:r_pe_cs3s}) paints a similar picture: with a significant value for the difficulty factor and no significance either for scrambling nor for scrambling and difficulty interaction.
	    We would conclude that in spite of our reduced sample sizes and aforementioned reproducibility issues our scrambling algorithm represents a viable type for a baseline (further discussion in section~\ref{sec:d_pe}.
    \vspace{0.2cm}
    \begin{center}
    \textbf{Henceforth we are dealing with multimodal data obtained in our optometry-fMRI set-up (described under sections~\ref{sec:m_fmri} and~\ref{sec:m_om}):}
    \end{center}
    
    \section{Participant Response Analysis}\label{sec:r_ra}
	We are testing parameters of the participant input in our main experiments similarly to how we have assessed our preliminary trials.
	This is to determine the extent to which we can rely on our baseline in in the further analysis of this data set.

	Due to equipment malfunction participant input data is only available for 8 of our 12 participants.
	\subsection{Reaction Times}\label{sec:r_ra_rt}
	    \py[ra_rt]{fig_ra_rt}
	    \py[ra_rt]{lm(data=data_ra_rt[(data_ra_rt['difficulty'] == 'easy') | (data_ra_rt['difficulty'] == 'hard')].reset_index(), fixed='RT~difficulty*scrambled', random='ID', title='Mixed model fitted to the raw reaction times depicted in figure~\\ref{fig:fig:ra_rt}. Our factors are scrambling (yes or no) and difficulty (easy or hard). The model chooses "easy" and "not scrambled" as the intercept value. Difficulty shows a significant positive effect, which is to be expected. Scrambling also shows a significant positive effect - meaning that pattern recognition trials have higher reaction times. This reaction time difference is also not a global offset as difficulty and scrambling interact to form a significant negative effect.', label='tab:r_ra_rt')}
	    The reaction times for our experiment are shown in figure~\ref{fig:ra_rt}.
	    An obvious feature (especially in comparison to the reaction time relationships we sought to find in section~\ref{sec:r_pe}) is the highly significant difference between easy emotion recognition and “easy” pattern recognition -
	    two tailed paired t-test \textit{p}-value of \py[ra_rt]{tex_nr(ttest_rel(data_ra_rt[(data_ra_rt['difficulty'] == "easy") & (data_ra_rt['emotion'] == "scrambled")].groupby(level=0)['RT'].mean(), data_ra_rt[(data_ra_rt['difficulty'] == "easy") & (data_ra_rt['emotion'] != "scrambled")].groupby(level=0)['RT'].mean())[1]/2)}.
	    
	    To further describe this potentially disruptive development for our baseline, we have fitted a mixed model with reaction time as our dependent variable, and emotion and difficulty as our factors (independent variables).
	    Overall, the model listed in table~\ref{tab:r_ra_rt} gives a quantitative description which accurately describes that pattern recognition trials prompted significantly higher reaction times.
	    Also, the emergence of a significant interaction factor in the model indicates that among the easy and hard pattern recognition trials, the difference in reaction times does not correspond directly to what the effect of the difficulty factor prescribes.
	    These observations on a whole prompt caution in further utilisation of our baseline and predict limited reliability of results thus obtained.
	\subsection{Error Rates}\label{sec:r_ra_er}
	    Additionally we have performed an error rate analysis to check for baseline weaknesses as seen in section~\ref{sec:r_pe_cs}.
	    \py[ra_er]{fig_ra_er}
	    \py[ra_er]{av(data_ra_er.reset_index(), 'ER~difficulty*scrambled+Error(ID)', title='One-way repeated measure ANOVA table for the data depicted in figure~\\ref{fig:ra_er}. The $Pr(>F)$ value indicates the adjusted probability observed results occurring by chance if the means of our defined categories are equal.', label='tab:r_ra_er')}
	    
	    Both figure~\ref{fig:ra_er} and the ANOVA test run on the portrayed data (under table~\ref{tab:r_ra_er}) show that the baseline is not robust in terms of error rates.
	    Note that the highly significant effect observed for the difficulty factor is not a weakness in the model per se - changes in error rates over difficulty alone would be acceptable and expected.
	    The significant interaction effect of difficulty and scrambling, however, indicates that the given difficulty effect on emotion recognition trials is not reproduced by pattern recognition trials, thus coming to the detriment of the pattern recognition baseline.	    
    
    \section{Pupillometry}\label{sec:r_p}
	\subsection{Methodological Reliability}\label{sec:r_p_mr}
	    Seeing as we are performing the debut pupillometric study on our present set-up, we have decided to perform at least a rough assessment of the quality of our measurements.
	    A number of possible inaccuracies became apparent during data acquisition (described in detail under section~\ref{sec:m_om_pm}) - 
	    and we are addressing these accordingly in the following paragraphs.
	    
	    To reiterate, it seemed concerning that Y-axis pupil diameter is often obfuscated by eyelid coverage.
	    Seeing as X-axis diameter incurred no such interference, we believe a simple analysis of correlation can show in how far our measurements are congruent and in how far the Y-axis measurements are reliable.
	    
	    Figure~\ref{fig:p_mr} shows raw data points and means for pupil diameters.
	    Due to the overwhelming amount of raw data points sharing the same variance, Pearson's \textit{r} is numerically undefined for our raw data.
	    We are thus fitting the correlation to the data point means. 
	    Here we see a Pearson's \textit{r} of \py[p_mr]{tex_nr(data_p_mr[0][0])} and a \textit{p}-value for non-correlation of approximately \py[p_mr]{tex_nr(data_p_mr[0][1])}.
	    It is safe to conclude from this result that our measurements are congruent.
	    We will thus be using the mean of X-axis and Y-axis diameters as our preferred diameter approximation.
	    
	    The observations we made during data acquisition pertaining to interference from the eyelids with Y-axis measurements are however also confirmed to be true.
	    A slight offshoot of raw data points is visible for higher values underneath the regression curve.
	    This very probably represents single measurements during which the eyelid partially covered the pupil.  
	    while this offshoot is obscured for mean values, its effect can be traced in the linear formula of the regression line: 
	    $y = $\py[p_mr]{tex_nr(data_p_mr[1])}$x$\py[p_mr]{tex_nr(data_p_mr[2])}.
	    The \textit{y}-intercept (\py[p_mr]{tex_nr(data_p_mr[2])}) indicates that there is a fixed offset between X-axis and Y-axis measurements, by which across our measurement range (approx $[20;75]$) Y-axis values are smaller than X-axis values.
	    \py[p_mr]{fig_p_mr}
\chapter{Discussion}
    \section{Preliminary Experiments}\label{sec:d_pe}
	Most conclusions drawn from our preliminary experiments (for instance in section~\ref{sec:m_pe_cs} or section~\ref{sec:r_pe_cs}) are only marginally reliable due to the high inter-run variability noticed throughout our re-runs in section~\ref{sec:r_pe}.
	We believe the chief reason for this is our reduced participant sample (n = 6 to 7), and we would recommend a more thorough examination on a larger population to better support any claims pertaining to accurate reaction time and error rate comparisons.
	
	Some hallmarks, however, have been constant over all our runs, and we believe these deserve to be reported with more pronounced certainty.
	\subsection{Reaction Times over Scrambling Cluster Sizes}
	    Our preliminary experiments show that even when confronted with very high variability in results there is a robust decay of reaction times as scrambling cluster sizes increase.
	    We report this is happening both with composite scrambling and with simple cluster-based scrambling.
	    
	    Though this effect was assumed by us a priori, and is hardly surprising, we hold that it validates our scrambling software for ample uses in Neuropsychology.
    \section{Pyxrand}
    A number of studies\cite{Rakover2013} have used scrambled face stimuli for their experiments.
    The stimuli are often prepared either by hand, or through time-consuming per-picture editing in an image manipulation program.
    Though automated scrambling software already exists \cite{Conway2008}, its scope is a lot narrower and its algorithms are nor released publicly.
    
    In findings 
    
    We recommend our software over possible alternatives due to its time-efficient batch functionality and its capacity to scramble both with and without specifically distorting facial features.
\input{acknowledgements.tex}
\input{supplementary.tex}
\input{footer.tex}
